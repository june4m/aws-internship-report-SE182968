[{"uri":"https://dokang307.github.io/Internship_Report/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Vietnam Cloud Day Event Objectives Equip executive leadership with strategic insights to navigate the Generative AI era. Disseminate best practices for establishing a unified, scalable data foundation on AWS. Present the AI-Driven Development Lifecycle (AI-DLC) and its transformative role in software engineering. Examine core security principles for Generative AI and the evolution toward autonomous AI Agents. Speakers Eric Yeo ‚Äì Country GM, Vietnam, Cambodia, Laos \u0026amp; Myanmar, AWS Dr. Jens Lottner ‚Äì CEO, Techcombank Ms. Trang Phung ‚Äì CEO \u0026amp; Co-Founder, U2U Network Jaime Valles ‚Äì VP \u0026amp; GM APJ, AWS Panelists: Jeff Johnson, Vu Van (ELSA), Nguyen Hoa Binh (Nexttech), Dieter Botha (TymeX) AWS Specialists: Kien Nguyen, Jun Kai Loke, Tamelly Lim, Binh Tran, Taiki Dang, Michael Armentano Key Highlights 1. Strategic Leadership \u0026amp; Vision Keynote Sessions: Leaders from AWS, Techcombank, and U2U Network outlined their vision for cloud and AI adoption across the region. Executive Panel: A discussion titled \u0026ldquo;Navigating the GenAI Revolution\u0026rdquo; focused on building an innovation-centric culture, aligning AI with business strategy, and managing organizational change during AI integration. 2. Data Foundation \u0026amp; Roadmap Unified Data Foundation: This session detailed the construction of a robust infrastructure for data ingestion, storage, and governance‚Äîa mandatory prerequisite for effective AI workloads. GenAI Roadmap: AWS showcased its comprehensive vision and emerging tools designed to empower organizations to drive efficiency through GenAI. 3. The Future of Software Development AI-Driven Development Lifecycle (AI-DLC): Discussions centered on a shift where AI acts not merely as an assistant but as a central collaborator. This model combines AI execution with human oversight to accelerate innovation beyond traditional methodologies. 4. Security \u0026amp; Advanced Automation Securing GenAI: Security was addressed across three layers: infrastructure, models, and applications, emphasizing encryption, zero-trust architecture, and granular access controls. AI Agents: The event concluded with a focus on the shift from basic automation to Intelligent Agents‚Äîsystems capable of learning, adapting, and executing complex tasks autonomously. Key Takeaways Cultural Shift Adopting AI-DLC: Software development is evolving from human-led efforts with AI assistance to AI-centric collaboration, requiring teams to adapt their coding and testing approaches. Agents vs. Automation: There is a fundamental difference between static automation scripts and dynamic AI Agents that can make decisions based on changing inputs. Technical Pillars Data First: A unified and governed data foundation is critical for GenAI success. Security by Design: Security measures must be continuous and layered to ensure data confidentiality throughout the AI lifecycle. Applying to Work Assess Data Readiness: Evaluate the current AWS data infrastructure to ensure it meets the scalability and governance standards required for GenAI (referencing the Unified Data Foundation session). Explore AI Agents: Identify complex manual operations that are suitable for offloading to autonomous AI Agents rather than simple scripts. Adopt AI-DLC: Experiment with integrating AI tools more deeply into the development lifecycle, treating them as collaborators rather than just code completion utilities. Event Experience The summit offered a holistic perspective on the GenAI landscape, effectively balancing high-level strategy with technical depth.\nStrategic Insight: The panel featuring leaders from ELSA, Nexttech, and TymeX provided valuable real-world context on managing the cultural impact of AI. Technical Depth: The afternoon tracks were highly relevant, particularly the deep dives into AI-DLC and Securing GenAI, which directly align with our technical roadmap. Some event photos Add your event photos here\n"},{"uri":"https://dokang307.github.io/Internship_Report/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Varsity Yearbooks Overcomes Market Challenges Through SaaS and CleanSlate Authors: Bill Tarr, Anthony McClure, Brad Laughlin, and Rowena Branch\nPublication Date: March 13, 2025\nServices: Americas, AWS Well-Architected, AWS Well-Architected Framework, AWS Well-Architected Tool, Best Practices, Customer Solutions, DevOps, Education, Experience-Based Acceleration, Migration Solutions, SaaS\nby Brad Laughlin, VP of Strategy and Operations ‚Äì CleanSlate Technology Group\nBy Rowena Branch, SaaS Partner Manager ‚Äì AWS SaaS Factory\nBy Anthony McClure, Senior Partner Solutions Architect ‚Äì AWS SaaS Factory\nBy Bill Tarr, Principal Solutions Architect ‚Äì AWS SaaS Factory\nSoftware providers have evolved to survive. This has never been more true for education services during the 2020 pandemic. The changing needs of the institutions they serve pushed companies like Varsity Yearbooks, a leading provider of graduation products since the 1920s, to focus on faster innovation and shorter time-to-market through a SaaS delivery model.\nIn partnership with AWS SaaS Competency Partner CleanSlate Technology Group, Varsity Yearbooks carried out a strategic transformation of their products to adapt to the new online landscape, beginning with their yearbook line. This collaboration resulted in a SaaS transformation that reshaped Varsity Yearbooks‚Äô approach by adopting a SaaS mindset to drive revenue and market share.\nAccording to Varsity Yearbooks, they were simply trying to stay afloat before building a SaaS solution. CleanSlate helped them shift into a SaaS-oriented mindset, which ultimately increased revenue and market share. The result provided Varsity with a solution they felt completely confident in, enabling them to focus on the future and explore what comes next. Today, their team prioritizes delivering features to market within weeks instead of months and is achieving rapid market growth. The SaaS solution allowed Varsity Yearbooks to reimagine the yearbook production process and opened up possibilities for future markets. The team now constantly asks, ‚ÄúWhat else is possible?‚Äù\nCleanSlate‚Äôs work contributed to their recognition at the TechPoint Digital Transformation Awards for successfully helping Varsity Yearbooks thrive in the $1.9 trillion education services industry.\nCleanSlate has extensive experience designing and building SaaS solutions for customers through a comprehensive approach to designing and developing modern applications. CleanSlate‚Äôs expertise includes product design, DevOps automation, cloud-native and mobile development. Their core values drive their success, with the ultimate goal of leaving customers in a better position than when they started. Their team of technologists is inspired to innovate and deliver. By collaborating with clients on modernization efforts, they help identify SaaS opportunities and guide organizations to think differently. Customers must ask how a modern application built using the SaaS model can bring greater value, such as becoming a growth driver, revenue engine, superior customer experience platform, or accelerating feature delivery. Thinking differently about modernization and transforming it into SaaS and a product strategy is what differentiates CleanSlate Technology Group from its competitors.\nSolution Overview CleanSlate‚Äôs collaboration with Varsity Yearbooks was driven by the imminent risk of failure of their electronic yearbook design software. The project focused on modernizing and transforming their 14-year-old platform and infrastructure into a modern SaaS solution, while re-platforming the legacy system to run in parallel until the new SaaS MVP (Minimum Viable Product) was released.\nBy leveraging AWS services and the AWS SaaS Factory framework, we successfully modernized Varsity‚Äôs legacy software into a true SaaS product, as illustrated in Figure 1. CleanSlate‚Äôs modernization approach applied a SaaS product mindset, combining business planning with modern cloud-native application development. The multi-tenant solution designed and built by the team enabled business growth, competitive advantage, and enhanced customer flexibility and creativity.\n![][image1]\nFigure 1: Architecture model enabling SaaS delivery\nCleanSlate evaluated the overall solution from both a business and technology perspective.\nBusiness Perspective CleanSlate collaborated with Varsity Yearbooks to achieve their business goals, creating a marketable AWS SaaS solution. This process included building a product roadmap and go-to-market strategy, defining commercialization and packaging, establishing customer success metrics, using an MVP approach, and prioritizing modern UX/UI to improve usability and functionality.\nTechnology Perspective CleanSlate developed the technical foundation in alignment with Varsity Yearbooks‚Äô business vision, delivering a well-built and future-proof solution. This included profiling, analytics, identity/access management, tenant isolation/multi-tenant storage, SaaS-focused DevOps automation, microservice architecture design, partner integrations, and modernization using AWS, Angular, and Java.\nCleanSlate built a scalable SaaS application using more than 20 AWS services, including a multi-tenant architecture with AWS Identity and Access Management (IAM) and microservices for seamless tenant connectivity. Amazon Elastic Container Service (ECS) and Amazon Relational Database Service (RDS) enabled automatic horizontal scaling, while auto-scaling rules handled demand spikes. Application profiling, analytics, and monitoring tools such as AWS CloudTrail, New Relic, and Splunk ensured governance and scalability.\nThe SaaS DevOps approach automated the entire infrastructure with Hashicorp Terraform, integrated quality checks, and continuous integration/deployment pipelines. This enabled Varsity Yearbooks to scale their development team, run multiple environments simultaneously, and accelerate production releases with on-demand deployment.\nInitial challenges included operational inefficiencies hindering innovation, slow time-to-market impacting customer satisfaction, limited scalability and reliability, long bug resolution cycles, lack of monitoring/logging tools, and manual onboarding processes. CleanSlate addressed these challenges through modernization and a new SaaS product mindset.\nLessons Learned Overall, companies like CleanSlate must embody a passionate partnership mindset, demonstrating empathy toward customer challenges and working tirelessly to solve them with sustainable, long-term solutions. CleanSlate‚Äôs commitment to building a culture of service quality helps customers resolve both short-term and long-term challenges through improvements in processes, technology, and people. Partners must strive to understand the unique business needs of their customers, build trust, and collaborate to deliver the best solutions.\nFrom a technology perspective, leveraging SaaS Factory frameworks and performing AWS Well-Architected Reviews throughout the process truly helps companies succeed by building and deploying high-quality projects that take full advantage of the cloud. Investing in SaaS component design and strong SaaS DevOps principles enables sustainable and transformative SaaS product development.\nThe biggest lesson from this large-scale digital transformation is that change is genuinely difficult to accept, and it requires the right partnership to embrace it. Varsity Yearbooks and CleanSlate encountered several challenges but ultimately built trust, integrated quality into their processes, advised effectively, adapted well, and worked tirelessly to meet expectations ‚Äî all of which fueled success.\nConclusion The successful transformation of Varsity Yearbooks demonstrates how traditional businesses can reinvent themselves through strategic SaaS adoption. Through collaboration with CleanSlate and AWS, what began as a pandemic-driven necessity evolved into a transformative business opportunity, enabling faster feature delivery, improved scalability, and greater operational efficiency. This shift positioned Varsity Yearbooks for long-term growth and continuous innovation.\nThis success story highlights the value of AWS SaaS Competency Partners, who bring deep expertise in designing and building cloud-native solutions on AWS. Just as CleanSlate has helped organizations ease the burden of legacy migrations and establish strong foundations for SaaS solutions on AWS, they also provide comprehensive architectural and deployment support for enterprises pursuing digital transformation.\n![][image2]\nCleanslate Technology ‚Äì Featured Partner At CleanSlate Technology Group, we specialize in designing and building innovative SaaS products, data-driven solutions, and AI-powered applications that help clients achieve their business goals. As a trusted AWS partner, we provide cloud-native application development, DevOps services, and deep architectural expertise to unlock the full potential of modern technologies.\nOur expertise also includes migrating and modernizing legacy applications, ensuring they are future-ready and optimized for cloud environments. Whether you are looking to build scalable SaaS solutions, harness the power of data and AI, or modernize your infrastructure, we are committed to delivering transformative outcomes that drive growth, innovation, and long-term adaptability for your organization.\nContact CleanSlate Technology | CleanSlate Overview\n"},{"uri":"https://dokang307.github.io/Internship_Report/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"AWS Achieves Cyber Essentials Plus Certification for 2025 Author: Tariro Dongo\nDate: April 7, 2025\nServices: Announcements, Foundational (100), Security, Identity, \u0026amp; Compliance\nAmazon Web Services (AWS) is pleased to announce the successful renewal of its UK Cyber Essentials Plus certification. The Cyber Essentials Plus certificate is valid for one year, through March 21, 2026.\nCyber Essentials Plus is a certification program backed by the UK Government and industry, designed to help organizations demonstrate their cybersecurity posture in defending against common cyber threats. An independent third-party auditor certified by the Information Assurance for Small and Medium Enterprises (IASME) completed the audit. The scope of our Cyber Essentials Plus certification covers the AWS internal corporate network in the UK and Ireland.\nAWS‚Äôs compliance status is available through (1) the IASME website by searching for ‚ÄúAmazon Web Services,‚Äù (2) the AWS Cyber Essentials Plus compliance page, and (3) AWS Artifact. AWS Artifact is a self-service portal providing on-demand access to AWS compliance reports. Log in to AWS Artifact in the AWS Management Console or learn more at Getting Started with AWS Artifact.\nAWS continuously strives to enhance its compliance programs to help you meet your architectural and regulatory needs. Contact your AWS account team if you have any questions.\nTo learn more about our compliance and security programs, visit AWS Compliance Programs. As always, we welcome your feedback and questions‚Äîplease reach out to the AWS Compliance team through the Contact Us page.\nAbout the Author Tariro Dongo\nTari is a Security Assurance Program Manager at AWS based in London. She is responsible for third-party and customer audits, certifications, attestations, and assessments across the EMEA region. Prior to joining AWS, Tari worked in Security Assurance and Technology Risk within the Big Four and the financial services industry.\n"},{"uri":"https://dokang307.github.io/Internship_Report/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Weekly AWS Roundup: Amazon EKS, Amazon OpenSearch, Amazon API Gateway, and More (April 7, 2025) Author: S√©bastien Stormacq\nPublication Date: April 7, 2025\nServices: Amazon API Gateway, Amazon Elastic Kubernetes Service, Amazon Neptune, Amazon OpenSearch Service, Amazon Q Developer, Amazon Security Lake, Amazon Simple Email Service (SES), Announcements, AWS Identity and Access Management (IAM), Launch, News, Resource Access Manager (RAM)\nThe AWS Summit season kicks off this week! These free events are now being held around the world, bringing our cloud community together to connect, collaborate, and learn. Whether you prefer to join online or attend in person, these gatherings offer valuable opportunities to deepen your AWS knowledge.\nI will be attending the Summit in Paris this week ‚Äî the largest cloud conference in France ‚Äî and the Summit in London later this month. We‚Äôll have a small podcast studio üéôÔ∏è where I‚Äôll interview French and UK customers to produce new episodes of the AWS Developers Podcast and le podcast üéô AWS ‚òÅÔ∏è en üá´üá∑.\nRegister today!\nBut first, let‚Äôs take a look at last week‚Äôs announcements.\nLaunches from Last Week At KubeCon London, we announced the EKS Community Add-Ons Catalog, making it easier for Kubernetes users to enhance their Amazon EKS clusters with powerful open-source tools. The catalog simplifies installation of essential add-ons such as metrics-server, kube-state-metrics, prometheus-node-exporter, cert-manager, and external-dns. By integrating these community-developed add-ons directly into the Amazon EKS console and the AWS Command Line Interface (CLI), customers can reduce operational complexity and accelerate deployment while maintaining flexibility and security. This release reflects AWS‚Äôs commitment to the Kubernetes community by offering seamless access to trusted open-source solutions without the burden of manual installation and maintenance.\nAmazon Q Developer is now integrated with Amazon OpenSearch Service to enhance operational analytics by enabling natural language exploration and AI-powered data visualization. This integration simplifies querying and visualizing data, reducing the challenges associated with traditional query languages and tools. During incident response, Amazon Q Developer provides contextual summaries and insights directly in the alert interface, enabling faster analysis and resolution. This enhancement allows engineers to focus more on innovation by streamlining troubleshooting and improving monitoring infrastructure.\nAmazon API Gateway now supports dual-stack endpoints (IPv4 and IPv6) across all endpoint types, custom domain names, and managed APIs in both commercial regions and AWS GovCloud (US). This enhancement enables REST, HTTP, and WebSocket APIs‚Äîas well as custom domains‚Äîto receive requests from both IPv4 and IPv6 clients, easing IPv6 migration and addressing IPv4 address exhaustion. This update builds on recent IPv6-focused improvements, including AWS Identity and Access Management (IAM) introducing dual-stack public endpoints for seamless connectivity across both protocols, and AWS Resource Access Manager (RAM) enabling resource sharing using IPv6 addresses. Additionally, Amazon Security Lake customers can now use IPv6 through new dual-stack endpoints to configure and manage the service. Together, these enhancements ensure broader compatibility and prepare network infrastructure for the future.\nAmazon SES has introduced attachment support in its v2 APIs, allowing users to attach files such as PDFs and images directly to their emails without manually generating MIME messages. This improvement simplifies sending rich email content and reduces implementation complexity. Amazon Simple Email Service (Amazon SES) supports attachments in all AWS Regions where the service is available.\nAmazon Neptune has updated its Service Level Agreement (SLA) to provide 99.99% monthly uptime for Multi-AZ DB Instance, Multi-AZ DB Cluster, and Multi-AZ Graph, up from the previous 99.9%. This enhancement reflects AWS‚Äôs commitment to delivering highly available and reliable graph database services for mission-critical applications. The improved SLA is now available in all AWS Regions where Amazon Neptune is offered.\nFor a full list of AWS announcements, keep an eye on the What\u0026rsquo;s New at AWS page.\nOther AWS Events Check your calendar and register for upcoming AWS events.\nAWS GenAI Lofts are collaborative, immersive spaces showcasing AWS‚Äôs cloud and AI expertise. They give startups and developers the opportunity to experience AI products and services firsthand, attend exclusive sessions with industry leaders, and network with investors and peers. Find a GenAI Loft location near you and don‚Äôt forget to register.\nBrowse all upcoming AWS in-person and online events here.\nThat‚Äôs all for this week. Come back next Monday for the next Weekly Roundup!\n‚Äî seb\nThis post is part of our Weekly Roundup series. Visit each week for a quick recap of exciting AWS news and announcements!\nBlog News: How does it work? Take this 1-minute survey!\n(This survey is hosted by an external company. AWS processes your information as described in the AWS Privacy Notice. AWS will own the data collected through this survey and will not share participants‚Äô information.)\nAbout the Author S√©bastien Stormacq\nSeb has been writing code since he first touched a Commodore 64 in the mid-1980s. He inspires developers to unlock the value of the AWS Cloud with a unique mix of passion, enthusiasm, customer obsession, curiosity, and creativity.\nHe cares deeply about software architecture, developer tools, and mobile computing.\nIf you want to sell him something, make sure it has an API.\nFollow @sebsto on Bluesky, X, Mastodon, and other platforms.\n"},{"uri":"https://dokang307.github.io/Internship_Report/3-blogstranslated/3.4-blog4/","title":"Blog 4","tags":[],"description":"","content":"Getting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://dokang307.github.io/Internship_Report/3-blogstranslated/3.5-blog5/","title":"Blog 5","tags":[],"description":"","content":"Getting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://dokang307.github.io/Internship_Report/3-blogstranslated/3.6-blog6/","title":"Blog 6","tags":[],"description":"","content":"Getting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://dokang307.github.io/Internship_Report/5-workshop/5.4-s3-onprem/5.4.1-prepare/","title":"Create Lambda Functions","tags":[],"description":"","content":"Step 1: Create IAM Role for Lambda Go to IAM Console ‚Üí Roles ‚Üí Create role\nTrusted entity type:\nAWS service Use case: Lambda Add permissions:\nAWSLambdaVPCAccessExecutionRole AWSLambdaBasicExecutionRole Role details:\nRole name: daivietblood-lambda-role Description: IAM role for DaiVietBlood Lambda functions Click Create role\nStep 2: Create Lambda Layer for Dependencies Create a folder for dependencies: mkdir -p nodejs cd nodejs npm init -y npm install mysql2 Create zip file: cd .. zip -r mysql2-layer.zip nodejs Go to Lambda Console ‚Üí Layers ‚Üí Create layer\nConfigure:\nName: mysql2-layer Upload: Select mysql2-layer.zip Compatible runtimes: Node.js 18.x, Node.js 20.x Click Create\nStep 3: Create Lambda Function - Get Users Go to Lambda Console ‚Üí Functions ‚Üí Create function\nBasic information:\nFunction name: daivietblood-get-users Runtime: Node.js 20.x Architecture: x86_64 Execution role: Use existing role ‚Üí daivietblood-lambda-role Click Create function\nAdd Layer:\nScroll to Layers ‚Üí Add a layer Custom layers ‚Üí Select mysql2-layer Click Add Configure VPC:\nGo to Configuration ‚Üí VPC ‚Üí Edit VPC: daivietblood-vpc Subnets: Select both Private Subnets Security groups: daivietblood-lambda-sg Click Save Add Environment Variables:\nGo to Configuration ‚Üí Environment variables ‚Üí Edit Add: DB_HOST = daivietblood-db.xxxx.ap-southeast-1.rds.amazonaws.com DB_PORT = 3306 DB_NAME = daivietblood DB_USER = admin DB_PASSWORD = YourSecurePassword123! Click Save Add code in Code tab:\nconst mysql = require(\u0026#39;mysql2/promise\u0026#39;); let connection; const getConnection = async () =\u0026gt; { if (!connection) { connection = await mysql.createConnection({ host: process.env.DB_HOST, port: process.env.DB_PORT, user: process.env.DB_USER, password: process.env.DB_PASSWORD, database: process.env.DB_NAME }); } return connection; }; exports.handler = async (event) =\u0026gt; { try { const conn = await getConnection(); const [rows] = await conn.execute(\u0026#39;SELECT * FROM users\u0026#39;); return { statusCode: 200, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify(rows) }; } catch (error) { console.error(\u0026#39;Error:\u0026#39;, error); return { statusCode: 500, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ error: \u0026#39;Internal server error\u0026#39; }) }; } }; Click Deploy Step 4: Create Lambda Function - Create User Create new function: daivietblood-create-user Same configuration as above (VPC, Layer, Environment Variables) Add code: const mysql = require(\u0026#39;mysql2/promise\u0026#39;); let connection; const getConnection = async () =\u0026gt; { if (!connection) { connection = await mysql.createConnection({ host: process.env.DB_HOST, port: process.env.DB_PORT, user: process.env.DB_USER, password: process.env.DB_PASSWORD, database: process.env.DB_NAME }); } return connection; }; exports.handler = async (event) =\u0026gt; { try { const body = JSON.parse(event.body); const { email, name, blood_type, phone } = body; if (!email || !name || !blood_type) { return { statusCode: 400, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ error: \u0026#39;Missing required fields\u0026#39; }) }; } const conn = await getConnection(); const [result] = await conn.execute( \u0026#39;INSERT INTO users (email, name, blood_type, phone) VALUES (?, ?, ?, ?)\u0026#39;, [email, name, blood_type, phone || null] ); return { statusCode: 201, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ id: result.insertId, email, name, blood_type, phone }) }; } catch (error) { console.error(\u0026#39;Error:\u0026#39;, error); if (error.code === \u0026#39;ER_DUP_ENTRY\u0026#39;) { return { statusCode: 409, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ error: \u0026#39;Email already exists\u0026#39; }) }; } return { statusCode: 500, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ error: \u0026#39;Internal server error\u0026#39; }) }; } }; Step 5: Create Lambda Function - Emergency Requests Create function: daivietblood-emergency-requests Same configuration Add code: const mysql = require(\u0026#39;mysql2/promise\u0026#39;); let connection; const getConnection = async () =\u0026gt; { if (!connection) { connection = await mysql.createConnection({ host: process.env.DB_HOST, port: process.env.DB_PORT, user: process.env.DB_USER, password: process.env.DB_PASSWORD, database: process.env.DB_NAME }); } return connection; }; exports.handler = async (event) =\u0026gt; { const conn = await getConnection(); const method = event.httpMethod; try { if (method === \u0026#39;GET\u0026#39;) { const [rows] = await conn.execute( \u0026#39;SELECT * FROM emergency_requests WHERE status = \u0026#34;open\u0026#34; ORDER BY urgency DESC, created_at DESC\u0026#39; ); return { statusCode: 200, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify(rows) }; } if (method === \u0026#39;POST\u0026#39;) { const body = JSON.parse(event.body); const { requester_name, blood_type, units_needed, hospital, urgency } = body; const [result] = await conn.execute( \u0026#39;INSERT INTO emergency_requests (requester_name, blood_type, units_needed, hospital, urgency) VALUES (?, ?, ?, ?, ?)\u0026#39;, [requester_name, blood_type, units_needed, hospital, urgency || \u0026#39;normal\u0026#39;] ); return { statusCode: 201, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ id: result.insertId, message: \u0026#39;Emergency request created\u0026#39; }) }; } return { statusCode: 405, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ error: \u0026#39;Method not allowed\u0026#39; }) }; } catch (error) { console.error(\u0026#39;Error:\u0026#39;, error); return { statusCode: 500, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, body: JSON.stringify({ error: \u0026#39;Internal server error\u0026#39; }) }; } }; Verification Checklist IAM Role created with VPC and Basic execution permissions Lambda Layer created with mysql2 package Lambda functions created and deployed: daivietblood-get-users daivietblood-create-user daivietblood-emergency-requests All functions configured with VPC (Private Subnets) Environment variables set correctly Functions deployed successfully "},{"uri":"https://dokang307.github.io/Internship_Report/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/","title":"Create VPC","tags":[],"description":"","content":"Step 1: Create VPC Go to VPC Console ‚Üí Your VPCs ‚Üí Create VPC\nConfigure VPC:\nResources to create: VPC and more Name tag auto-generation: daivietblood IPv4 CIDR block: 10.0.0.0/16 IPv6 CIDR block: No IPv6 CIDR block Tenancy: Default Configure Subnets:\nNumber of Availability Zones: 2 Number of public subnets: 2 Number of private subnets: 2 Customize subnets CIDR blocks: Public subnet CIDR block in ap-southeast-1a: 10.0.1.0/24 Public subnet CIDR block in ap-southeast-1b: 10.0.2.0/24 Private subnet CIDR block in ap-southeast-1a: 10.0.3.0/24 Private subnet CIDR block in ap-southeast-1b: 10.0.4.0/24 Configure NAT Gateway:\nNAT gateways: In 1 AZ Configure VPC Endpoints:\nVPC endpoints: None (we\u0026rsquo;ll create later if needed) Click Create VPC\n‚ÑπÔ∏è VPC creation takes 2-3 minutes. Wait until status shows \u0026ldquo;Available\u0026rdquo;.\nStep 2: Verify VPC Resources After creation, verify the following resources were created:\nResource Name Details VPC daivietblood-vpc 10.0.0.0/16 Public Subnet 1 daivietblood-subnet-public1-ap-southeast-1a 10.0.1.0/24 Public Subnet 2 daivietblood-subnet-public2-ap-southeast-1b 10.0.2.0/24 Private Subnet 1 daivietblood-subnet-private1-ap-southeast-1a 10.0.3.0/24 Private Subnet 2 daivietblood-subnet-private2-ap-southeast-1b 10.0.4.0/24 Internet Gateway daivietblood-igw Attached to VPC NAT Gateway daivietblood-nat-public1-ap-southeast-1a In Public Subnet 1 Route Table (Public) daivietblood-rtb-public Routes to IGW Route Table (Private) daivietblood-rtb-private1-ap-southeast-1a Routes to NAT Step 3: Create Security Groups 3.1. Security Group for Lambda\nGo to VPC Console ‚Üí Security Groups ‚Üí Create security group\nConfigure:\nSecurity group name: daivietblood-lambda-sg Description: Security group for Lambda functions VPC: Select daivietblood-vpc Inbound rules: (Leave empty - Lambda initiates connections)\nOutbound rules:\nType Protocol Port Destination Description All traffic All All 0.0.0.0/0 Allow all outbound Click Create security group\n3.2. Security Group for RDS\nGo to VPC Console ‚Üí Security Groups ‚Üí Create security group\nConfigure:\nSecurity group name: daivietblood-rds-sg Description: Security group for RDS MySQL VPC: Select daivietblood-vpc Inbound rules:\nType Protocol Port Source Description MySQL/Aurora TCP 3306 daivietblood-lambda-sg Allow Lambda access Outbound rules:\nType Protocol Port Destination Description All traffic All All 0.0.0.0/0 Allow all outbound Click Create security group\n‚ö†Ô∏è Security Best Practice: Only allow access from Lambda Security Group to RDS. Never open port 3306 to 0.0.0.0/0.\nStep 4: Create DB Subnet Group Go to RDS Console ‚Üí Subnet groups ‚Üí Create DB subnet group\nConfigure:\nName: daivietblood-db-subnet-group Description: Subnet group for DaiVietBlood RDS VPC: Select daivietblood-vpc Add subnets:\nAvailability Zones: Select ap-southeast-1a and ap-southeast-1b Subnets: Select both Private Subnets (10.0.3.0/24 and 10.0.4.0/24) Click Create\nVerification Checklist VPC created with CIDR 10.0.0.0/16 2 Public Subnets created 2 Private Subnets created Internet Gateway attached to VPC NAT Gateway created in Public Subnet Route tables configured correctly Lambda Security Group created RDS Security Group created with inbound rule from Lambda SG DB Subnet Group created with Private Subnets "},{"uri":"https://dokang307.github.io/Internship_Report/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Data Resiliency in a Cloud-first World Event Objectives Share best practices in modern application design. Introduce DDD methods and event-driven architecture. Guide on selecting appropriate compute services. Introduce AI tools supporting the development lifecycle. List of Speakers Paul Haverfield - Principal Storage Specialist BDM, APJ Tamelly Lim - Specialist Solutions Architect Ameen Khan S - GTM specialist for Storage - Data \u0026amp; AI pillar covering ASEAN markets Paul Hidalgo - Key Highlights Today, I had the opportunity to attend an AWS program covering an extremely urgent topic in the current context: Data Resiliency. More than just simple backups, the event opened up new perspectives on protecting digital assets against increasingly sophisticated threats.\nHere are the key takeaways I gathered:\n1. Redefining: How is Data Resiliency different from High Availability (HA) and Disaster Recovery (DR)? Previously, we often focused on HA (ensuring systems are always online) or DR (recovering from physical disasters). However, Data Resiliency is a broader and more \u0026ldquo;proactive\u0026rdquo; concept:\nContext: \u0026ldquo;Everything fails, all the time\u0026rdquo; (Werner Vogels) ‚Äì Everything is prone to failure, including physical keys or hardware. The Difference: While HA handles infrastructure incidents, Data Resiliency focuses on data integrity. It is the organizational ability to maintain operations, withstand, and recover even when under cyberattack (such as Ransomware) or human error. Goal: To detect anomalies and automate response processes without human intervention. 2. Why has Data Resiliency become an \u0026ldquo;Absolute Necessity\u0026rdquo;? The explosion of data creation comes with new technological vulnerabilities. Three main trends are driving the shift from Protection to Resiliency:\nRegulatory: Compliance with strict data protection laws. Technology: The complexity of Multi-cloud and Hybrid-cloud environments. Threat Landscape: Ransomware no longer just encrypts primary data; it also targets backups. 3. Data Immutability: The Impenetrable Shield A keyword mentioned repeatedly was Data Immutability.\nThis refers to the ability to create data copies that cannot be changed or deleted for a set period. In the event of a Ransomware attack, even if hackers possess top-level admin rights, they cannot alter this backup. It acts as the \u0026ldquo;Last line of defense,\u0026rdquo; ensuring that at least one clean version always exists for recovery. 4. Protection Strategy: The AWS 3-2-1-1-0 Model The traditional 3-2-1 backup rule has been upgraded to suit the cloud era:\n3 copies of data. 2 different storage media. 1 off-site copy (different region). 1 offline or Immutable (Air-gapped) copy. 0 errors during recovery (verified by automated testing). Important concepts to remember:\nRPO (Recovery Point Objective): How much data loss is acceptable? RTO (Recovery Time Objective): How long does it take to get the system running again? Backup Vault: A container for storing backups, encrypted by AWS KMS for enhanced security. 5. Tool Ecosystem \u0026amp; Solutions The event introduced powerful integrated solutions on AWS:\nCommvault Cloud on AWS: Provides Air Gap Protect (secure data isolation). Cloud Rewind: The ability to \u0026ldquo;rewind\u0026rdquo; time to restore entire instances or VPCs as if the incident never happened. Clumio: An all-in-one simplified backup solution. Uses Serverless Workflow architecture (an army of Lambda functions) to optimize costs and operations. Elastio: Focuses on: Detect, Respond, Recover. Scans for Malware/Ransomware directly within Snapshots to ensure backups do not contain latent malicious code. 6. Workshop Architecture: Real-world Implementation During the hands-on session, we deployed a comprehensive protection model:\nKey Components:\nSource: EC2 Instances (EBS) and S3 Buckets containing critical data. Mechanism: Using AWS Backup Plan with an hourly schedule. Protection Layers: Primary: Stored in a standard Vault (workshop-sources-regular-vault). Secondary (Air-gapped): Copied to another Region (us-east-1-LAG-Vault) with Immutability settings enabled. Validation: Integrated Elastio with AWS Backup. Automated Malware scanning on backups. Performed hourly Restore Testing to ensure backup viability (the \u0026ldquo;0 error\u0026rdquo; strategy). Conclusion The event shifted my mindset from merely \u0026ldquo;backing up data\u0026rdquo; to \u0026ldquo;building resiliency.\u0026rdquo; In an era where cyberattacks are inevitable, possessing a Data Resiliency strategy featuring Immutability and Automation (using tools like Elastio or Commvault) is vital for business survival.\nSome photos from the event Add your photos here Overall, the event not only provided technical knowledge but also helped me change my thinking regarding application design, system modernization, and more effective team collaboration.\n"},{"uri":"https://dokang307.github.io/Internship_Report/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Data Science on AWS Event Objectives Share essential services for data processing (sentiment analysis, comment classification, etc.). List of Speakers Van Hoang Kha - Cloud Solutions Architect, AWS Community Builder Bach Doan Vuong - Cloud DevOps Engineer, AWS Community Builder Key Highlights Below is a summary of the event content focusing on the listed services, presented in a professional report style without icons as requested.\nAWS TRAINING PROGRAM SUMMARY REPORT: AI \u0026amp; MACHINE LEARNING\n1. Overview of Technology Concepts\nTo begin the program, we systematized important foundational concepts in the field of intelligent technology:\nAI (Artificial Intelligence): An overarching concept regarding the creation of intelligent systems. ML (Machine Learning): A subset of AI that allows computers to learn from data. DL (Deep Learning): Uses complex neural networks to model patterns in data. GenAI (Generative AI): Focuses on creating new content and data. 2. AWS as a Service Provider\nThe next section introduced AWS as a comprehensive service provider. AWS offers Managed Services that help businesses apply AI quickly without investing heavily in building infrastructure from scratch.\n3. Details of Introduced AWS Services\nThe training focused deeply on specific tools designed to solve real-world business problems:\nAmazon Comprehend (Natural Language Processing Service - NLP) This service was discussed in the most detail, featuring powerful multi-language text processing capabilities:\nSentiment Analysis: Automatically classifies customer reviews and comments based on positive, negative, or neutral nuances. Text Summarization: Condenses content from long documents. Large-scale Information Processing: Supports bulk email processing and classification. Information Security: Capable of identifying, classifying, and protecting sensitive Personally Identifiable Information (PII) within text. Other Language and Text Processing Services\nAmazon Translate: Automated language translation service. Amazon Textract: A tool for extracting data from scanned documents and papers, including handwriting and complex forms. Amazon Transcribe: A service for converting speech (audio) into written text. Image and Computer Vision Services\nAmazon Rekognition: A Deep Learning-based service specialized for analyzing images and videos (object detection, facial recognition, content moderation). Customer Experience Services\nAmazon Personalize: A solution to enhance customer experience through personalization. This service records and analyzes user behavior, thereby providing product or content recommendations best suited to individual preferences. Technical Infrastructure\nSageMaker Instance: Provides the server environment and tools necessary for developers to self-build, train, and deploy custom machine learning models according to specific needs. Some photos from the event Add your photos here Overall, the event not only provided technical knowledge but also helped me change my thinking regarding application design, system modernization, and more effective team collaboration.\n"},{"uri":"https://dokang307.github.io/Internship_Report/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":"Summary Report: Attending AWS Cloud Mastery Series #1 - AI/ML/GenAI on AWS\nAttendance Objectives Recently, I had the opportunity to attend the opening event for the \u0026ldquo;AWS Cloud Mastery\u0026rdquo; series, focusing on AI, Machine Learning, and Generative AI. My main goal was to update my comprehensive view of these technologies on the AWS platform and learn how to apply them to real-world business problems.\nSpeakers The session featured sharing from experienced experts in the industry, including Mr. Lam Tuan Kiet (Sr DevOps Engineer - FPT Software), Mr. Danh Hoang Hieu Nghi (AI Engineer - Renova Cloud), Mr. Dinh Le Hoang Anh (Cloud Engineer Trainee), and Mr. Van Hoang Kha (Community Builder).\nValuable Knowledge I Harvested:\n1. The Power of Generative AI on Amazon Bedrock This was the part that impressed me the most. Amazon Bedrock acts as a central platform, providing access to leading Foundation Models (FMs) from Anthropic, OpenAI, Meta, etc. This allows us to fine-tune existing models without having to build a model from scratch.\nI also reinforced my skills in Prompt Engineering, understanding better how to guide the model through various strategies:\nZero-shot: Providing a request directly without examples. Few-shot: Providing a handful of examples for the model to mimic. Chain-of-Thought: Asking the model to explain its reasoning steps for a more logical result. Specifically, the RAG (Retrieval Augmented Generation) technique was highlighted as an optimal solution to improve accuracy:\nRetrieval: Pulling real data from the enterprise knowledge base. Augmentation: Adding that data as context for the prompt. Generation: The model answers based on factual information, minimizing hallucinations. Additionally, Amazon Titan Embeddings was introduced as a tool to convert text into vectors, serving semantic search and multilingual RAG workflows.\n2. AWS AI Services Ecosystem Beyond GenAI, I also reviewed AWS\u0026rsquo;s \u0026ldquo;ready-made\u0026rdquo; AI services (APIs) that help solve specific problems quickly without complex model training:\nImage/Video Analysis (Rekognition). Translation (Translate) and Speech-to-Text/Text-to-Speech (Transcribe, Polly). Data Extraction (Textract) and Natural Language Processing (Comprehend). Intelligent Search (Kendra) or Anomaly Detection (Lookout). The AMZPhoto face recognition demo visually illustrated how to integrate these services into a real product.\n3. Amazon Bedrock AgentCore ‚Äì Putting AI Agents into Practice This is a new framework helping to solve the problem of operating AI Agents at scale (production-ready). It supports long-term memory management, identity security, tool integration (like browsers, code interpreters), and most importantly, observability. This makes deploying frameworks like CrewAI or LangGraph safer and more effective on the AWS platform.\nPlan for Application at Work Based on what I learned, I plan to apply the following knowledge immediately:\nDeploy RAG \u0026amp; AgentCore: Propose and apply these to upcoming internal projects requiring GenAI features to increase accuracy and automation capabilities. Optimize Development Process: Use available AWS AI Services instead of building from scratch to shorten Time-to-market. Improve Model Performance: Apply advanced Prompt Engineering techniques to optimize output results for current AI tasks. Side Experience Not only did I absorb knowledge, but the event atmosphere was also extremely lively. I was lucky enough to reach the Top 6 in the Kahoot competition; however, I dropped off the leaderboard during the final questions. Nevertheless, it was still an interesting game that helped me reinforce the knowledge from this event.\n"},{"uri":"https://dokang307.github.io/Internship_Report/4-eventparticipated/4.5-event5/","title":"Event 5","tags":[],"description":"","content":"Summary Report: Attending AWS Cloud Mastery Series #2 ‚Äì DevOps on AWS Attendance Objectives Continuing the event series, I attended the second session focusing on DevOps. My goal was to master AWS services supporting DevOps, deepen my understanding of CI/CD pipeline design, Infrastructure as Code (IaC) concepts, as well as how to deploy and monitor containerized applications on the AWS platform.\nSpeakers The session gathered a strong lineup of speakers, including AWS Community Builders and experienced engineers:\nMr. Truong Quang Tinh ‚Äì Platform Engineer (TymeX) Mr. Bao Huynh, Nguyen Khanh Phuc Thinh, Tran Dai Vi, Huynh Hoang Long, Pham Hoang Quy, Nghiem Le (AWS Community Builders) Mr. Dinh Le Hoang Anh ‚Äì Cloud Engineer Trainee (First Cloud AI Journey) Valuable Knowledge I Harvested: 1. Building a DevOps Foundation The speakers helped me redefine that DevOps is not a job title but a mindset and habit of working. The core lies in:\nAutomating repetitive tasks. Sharing knowledge across teams. Continuously experimenting and learning. Measuring effectiveness with real data rather than assumptions. I also learned lessons about common pitfalls for beginners: avoid getting stuck in \u0026ldquo;tutorial hell\u0026rdquo; without starting real projects, and focus on personal progress rather than comparing oneself to others.\n2. Infrastructure as Code (IaC) in Practice This section broadened my horizon regarding IaC tools instead of sticking to a single one. The speakers provided a detailed comparison:\nCloudFormation: AWS\u0026rsquo;s native template tool. AWS CDK: For developers who prefer writing infrastructure using familiar programming languages. Terraform: The optimal choice for teams working on multi-cloud platforms. The most important message: Infrastructure built via Code (IaC) is significantly more consistent, maintainable, and secure compared to manual configuration (ClickOps).\n3. Containers and Deployment Models The content ranged from basics (Dockerfile, Image) to advanced AWS services:\nAmazon ECR: Storage and security scanning for images. Amazon ECS \u0026amp; EKS: Two popular container orchestration options. The comparison between ECS (simple, native) and EKS (powerful with Kubernetes) helped me know when to use which. AWS App Runner: A quick deployment solution without worrying about cluster management. 4. Monitoring and Observability A system cannot lack monitoring capabilities. I understood better the roles of:\nAmazon CloudWatch: The center for metrics, logs, and alarms. AWS X-Ray: A tracing tool helping visualize request flows and detect bottlenecks. Core lesson: Observability features must be designed from the very beginning, not added after the system is built.\nPlan for Application at Work Specifically, I plan to apply this knowledge to the team\u0026rsquo;s upcoming AI Chatbot Project:\nEstablish CI/CD Pipeline: Use AWS CodePipeline to automate the entire process from Build, Test, to Deploy. The goal is to ensure every code update is tested and pushed to production smoothly. Implement IaC: Use AWS CDK to define all resources (Lambda, API Gateway, DynamoDB, S3, IAM\u0026hellip;). This makes the system easy to reuse for different environments and scale quickly when needed. Applying this process will help the Chatbot project develop faster, minimize human error, and make operations easier.\nEvent Experience The session gave me a practical perspective on how modern businesses implement DevOps on AWS. Beyond theory, the real-world examples from speakers about CloudFormation, Terraform, or how to choose EKS/ECS were truly valuable.\nBeyond professional knowledge, this was also a good occasion for me to network with like-minded friends and learn \u0026ldquo;hard-earned\u0026rdquo; experiences from those who came before me.\n"},{"uri":"https://dokang307.github.io/Internship_Report/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Nguy·ªÖn C√¥ng Minh\nPhone Number: 0363401520\nEmail: congminh04062004@gmail.com | minhncse182968@fpt.edu.vn\nUniversity: FPT University HCM\nMajor: Software Engineering\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 28/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://dokang307.github.io/Internship_Report/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Goals: Environment setup. Understand basic AWS services, how to use the Console \u0026amp; CLI. Tasks to be implemented this week: Day Task Start Date Completion Date Reference 2 - Learn how to draw diagrams using AWS icons\n- Course preparation (install required packages: powershell, winget, go, hugo)\n- Learn how to write markdown and create a website with Hugo 08/09/2025 08/09/2025 3 - Getting started with AWS\n- What is AWS\n- AWS Infrastructure\n- Cost optimization and AWS Support\n- Create an AWS account 09/09/2025 09/09/2025 Creating your first AWS account 4 - Learn about Amazon Virtual Private Cloud (VPC)\n- VPC\n- Subnet\n- Route table\n- Elastic Network Interface (ENI)\n- VPC Endpoint\n- Internet Gateway\n- NAT Gateway 10/09/2025 10/09/2025 https://youtu.be/O5CIvG0Wt78\nhttps://youtu.be/DHYgA94SefE\nhttps://youtu.be/dHoYmQR7FYs 5 - VPC Security and Multi-VPC features\n- Security Group (SG)\n- Network Access Control List (NACL)\n- VPC Flow Logs\n- VPC Peering \u0026amp; Transit Gateway\n- VPC Peering\n- Transit Gateway 11/09/2025 11/09/2025 6 - VPN \u0026amp; Direct Connect\n- VPN Site-to-Site\n- VPN Client-to-Site\n- AWS Direct Connect\n- Elastic Load Balancer\n- Elastic Load Balancing\n- Network Load Balancer\n- Gateway Load Balancer 12/09/2025 12/09/2025 Week 1 achievement: Mastering Networking Fundamentals and Security Mindset Core Knowledge: Established a solid framework for understanding AWS Global Infrastructure and Amazon VPC.\nPractical Skills:\nSetup: Completed secure AWS account creation (utilizing IAM Users) and set up the documentation environment (Hugo/Markdown). VPC: Successfully created a VPC manually, including Subnets (Public/Private), IGW, and a functional NAT Gateway. Network Security: Learned to distinguish between Security Groups and NACLs and how to apply them effectively to control network traffic. Cloud Mindset: Began developing a cloud-native mindset focused on security (Least Privilege) and connection optimization (VPC Endpoints). Challenges: Calculating CIDR Blocks and accurately updating Route Tables during NAT Gateway and VPC Peering configurations required high precision and was initially confusing. Further practice is needed.\n"},{"uri":"https://dokang307.github.io/Internship_Report/1-worklog/","title":"Worklog","tags":[],"description":"","content":"This worklog documents my 12-week learning journey and technical implementation process throughout the AWS training and internship program. The objective of this program was to build a strong foundation in cloud computing, develop hands-on experience with AWS services, and ultimately design, deploy, and optimize a complete cloud-based project following AWS best practices. The entire program was completed over approximately three months, with each week focusing on a specific set of skills or an architectural milestone. My learning and implementation process included: Gaining familiarity with AWS fundamentals (IAM, VPC, EC2, S3, CloudFront, etc.)\nCompleting hands-on labs on security, cost optimization, and multi-factor authentication\nStrengthening knowledge of compute, storage, networking, and serverless services\nDesigning the system architecture for the capstone project\nImplementing best practices for performance, scalability, and cost efficiency\nBuilding, deploying, and finalizing the complete cloud-based solution\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Hands-on Labs MFA for Aws account, IAM, VPC, Cost Optimize\nWeek 3: Mastering Compute Fundamentals, Cross-Platform Deployment, Network Security\nWeek 4: Storage \u0026amp; Content Delivery, using tool AWS CLI\nWeek 5: Serverless Architecture Research, Capstone Project Initial\nWeek 6: Project Architecture \u0026amp; Design\nWeek 7: Finalizing Solution Design and Data Orientation\nWeek 8: Consolidating High-Performance \u0026amp; Cost-Optimized Architectures\nWeek 9: Challenges in Model Training and Data Quality\nWeek 10: Architecture Diagrams \u0026amp; Infrastructure Foundation\nWeek 11: Buil and deploy project\nWeek 12: System Finalization\n"},{"uri":"https://dokang307.github.io/Internship_Report/5-workshop/5.1-workshop-overview/","title":"Workshop Overview","tags":[],"description":"","content":"System Architecture The DaiVietBlood system uses a Serverless-First architecture on AWS Cloud, prioritizing scalability, security, and operational optimization.\nArchitecture Components 1. Network Infrastructure (VPC)\nComponent Description VPC Virtual Private Cloud with CIDR 10.0.0.0/16 Public Subnet Contains NAT Gateway, allows Internet access Private Subnet Contains Lambda, RDS - isolated from Internet NAT Gateway Allows Private Subnet resources to access Internet Internet Gateway Allows Public Subnet to communicate with Internet 2. Application \u0026amp; Data Layer\nService Role AWS Lambda Process business logic (CRUD operations, emergency requests) API Gateway Receive HTTP requests, route to Lambda Amazon RDS MySQL database storing user data, blood inventory Amazon S3 Store static files (images, documents) 3. Frontend \u0026amp; Distribution\nService Role AWS Amplify Host React application CloudFront CDN distributes content globally with low latency 4. DevOps \u0026amp; Monitoring\nService Role CodePipeline Automate CI/CD process CodeBuild Build and test source code CloudWatch Collect logs, metrics, set up alarms Data Flow User Request ‚Üí CloudFront ‚Üí Amplify (Frontend) ‚Üì API Gateway ‚Üì AWS Lambda (Private Subnet) ‚Üì Amazon RDS (Private Subnet) Security Model Network Isolation: RDS and Lambda in Private Subnet, no direct Internet access IAM Roles: Each service has minimum required permissions (Least Privilege) Data Encryption: At-rest (RDS, S3) and In-transit (HTTPS) Security Groups: Control inbound/outbound traffic for each resource Workshop Objectives After completing this workshop, you will be able to:\n‚úÖ Create VPC with proper network segmentation ‚úÖ Deploy RDS MySQL in Private Subnet ‚úÖ Build Lambda functions and expose via API Gateway ‚úÖ Configure S3 and CloudFront for static content ‚úÖ Deploy React app with Amplify ‚úÖ Set up CI/CD pipeline ‚úÖ Monitor with CloudWatch "},{"uri":"https://dokang307.github.io/Internship_Report/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Goals: Tasks to be implemented this week: Day Task Start Date Completion Date Reference 2 - Create AWS account and setup MFA (Multi-Factor Authentication)\n- Create admin group and admin user\n- Verification\n- Create budgets and manage cost budgets\n- Explore AWS Support Plans 15/09/2025 15/09/2025 - MFA for AWS account\n- Creating Admin Group and Admin User 3 Theory Research:\n- VPC (Virtual Private Cloud)\n- Subnet\n- Route Table\n- Internet Gateway\n- Security Group\n- Network Access Control List (Network ACLs)\nPractice:\n- Create VPC, subnet, Internet Gateway, Route Table, Security Group, EC2 Instance, NAT Gateway\n- Check connectivity (failed) ‚Üí Clean up resources 16/09/2025 16/09/2025 - Introduction to Amazon VPC\n- Subnets\n- Route Table\n- Internet Gateway\n- NAT Gateway 4 - Re-practice and check connectivity\n- Learn and practice setting up Hybrid DNS system with Route 53 17/09/2025 17/09/2025 Route 53 5 - Attend AWS Vietnam Cloud Day event 18/09/2025 18/09/2025 6 - Practice setting up VPC Peering\n- Setup Transit Gateway 19/09/2025 19/09/2025 - VPC Peering\n- Transit Gateway Week 2 achievement Account Security \u0026amp; Governance: Successfully implemented critical security steps like MFA and applied IAM best practices (using Admin User instead of Root). Setting up AWS Budgets is a good habit to maintain. VPC Deployment \u0026amp; Troubleshooting: I manually deployed a full VPC architecture (IGW, NAT Gateway) and, most importantly, experienced Troubleshooting. Connection failures (though time-consuming) were the most valuable lessons, helping me clearly understand the role of each component in Route Tables and Security Groups. Advanced Networking: Grasped how to connect extended networks (VPC Peering and Transit Gateway) and started approaching Hybrid Cloud via Route 53 Resolver. Industry Engagement: Attending AWS Vietnam Cloud Day provided real-world context and technology trends, bridging learned knowledge with industry applications. "},{"uri":"https://dokang307.github.io/Internship_Report/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/","title":"Create Amazon RDS","tags":[],"description":"","content":"Step 1: Create RDS MySQL Instance Go to RDS Console ‚Üí Databases ‚Üí Create database\nChoose database creation method:\nStandard create Engine options:\nEngine type: MySQL Engine version: MySQL 8.0.35 (or latest 8.0.x) Templates:\nFree tier (for workshop/testing) Settings:\nDB instance identifier: daivietblood-db Master username: admin Credentials management: Self managed Master password: YourSecurePassword123! Confirm password: YourSecurePassword123! ‚ö†Ô∏è Important: Save your password securely. You will need it to connect from Lambda.\nStep 2: Instance Configuration Instance configuration: DB instance class: db.t3.micro (Free tier eligible) Storage type: General Purpose SSD (gp2) Allocated storage: 20 GiB Storage autoscaling: Disable (for cost control) Step 3: Connectivity Connectivity:\nCompute resource: Don\u0026rsquo;t connect to an EC2 compute resource Network type: IPv4 Virtual private cloud (VPC): daivietblood-vpc DB subnet group: daivietblood-db-subnet-group Public access: No ‚ö†Ô∏è Important! VPC security group: Choose existing Existing VPC security groups: daivietblood-rds-sg Availability Zone: ap-southeast-1a Database port:\nDatabase port: 3306 Step 4: Database Authentication Database authentication: Password authentication Step 5: Additional Configuration Database options:\nInitial database name: daivietblood DB parameter group: default.mysql8.0 Option group: default:mysql-8-0 Backup:\nEnable automated backups: Yes Backup retention period: 7 days Backup window: No preference Encryption:\nEnable encryption: Yes (default) Monitoring:\nEnable Enhanced monitoring: No (to reduce cost) Maintenance:\nEnable auto minor version upgrade: Yes Maintenance window: No preference Deletion protection:\nEnable deletion protection: No (for workshop) Click Create database\n‚ÑπÔ∏è RDS creation takes 10-15 minutes. Wait until status shows \u0026ldquo;Available\u0026rdquo;.\nStep 6: Get RDS Endpoint After RDS is available:\nGo to RDS Console ‚Üí Databases ‚Üí Click daivietblood-db\nIn Connectivity \u0026amp; security tab, copy:\nEndpoint: daivietblood-db.xxxxxxxxxxxx.ap-southeast-1.rds.amazonaws.com Port: 3306 Save these values for Lambda configuration:\nDB_HOST=daivietblood-db.xxxxxxxxxxxx.ap-southeast-1.rds.amazonaws.com DB_PORT=3306 DB_NAME=daivietblood DB_USER=admin DB_PASSWORD=YourSecurePassword123! Step 7: Create Database Schema Since RDS is in Private Subnet, you need to connect via a bastion host or use Lambda to initialize the schema.\nOption A: Using Lambda to Initialize (Recommended)\nCreate a one-time Lambda function to initialize the database:\n// init-db.js const mysql = require(\u0026#39;mysql2/promise\u0026#39;); exports.handler = async (event) =\u0026gt; { const connection = await mysql.createConnection({ host: process.env.DB_HOST, user: process.env.DB_USER, password: process.env.DB_PASSWORD, database: process.env.DB_NAME }); // Create tables const createUsersTable = ` CREATE TABLE IF NOT EXISTS users ( id INT AUTO_INCREMENT PRIMARY KEY, email VARCHAR(255) UNIQUE NOT NULL, name VARCHAR(255) NOT NULL, blood_type ENUM(\u0026#39;A+\u0026#39;, \u0026#39;A-\u0026#39;, \u0026#39;B+\u0026#39;, \u0026#39;B-\u0026#39;, \u0026#39;AB+\u0026#39;, \u0026#39;AB-\u0026#39;, \u0026#39;O+\u0026#39;, \u0026#39;O-\u0026#39;) NOT NULL, phone VARCHAR(20), created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ) `; const createDonationsTable = ` CREATE TABLE IF NOT EXISTS donations ( id INT AUTO_INCREMENT PRIMARY KEY, user_id INT NOT NULL, donation_date DATE NOT NULL, location VARCHAR(255), status ENUM(\u0026#39;scheduled\u0026#39;, \u0026#39;completed\u0026#39;, \u0026#39;cancelled\u0026#39;) DEFAULT \u0026#39;scheduled\u0026#39;, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP, FOREIGN KEY (user_id) REFERENCES users(id) ) `; const createEmergencyRequestsTable = ` CREATE TABLE IF NOT EXISTS emergency_requests ( id INT AUTO_INCREMENT PRIMARY KEY, requester_name VARCHAR(255) NOT NULL, blood_type ENUM(\u0026#39;A+\u0026#39;, \u0026#39;A-\u0026#39;, \u0026#39;B+\u0026#39;, \u0026#39;B-\u0026#39;, \u0026#39;AB+\u0026#39;, \u0026#39;AB-\u0026#39;, \u0026#39;O+\u0026#39;, \u0026#39;O-\u0026#39;) NOT NULL, units_needed INT NOT NULL, hospital VARCHAR(255) NOT NULL, urgency ENUM(\u0026#39;critical\u0026#39;, \u0026#39;urgent\u0026#39;, \u0026#39;normal\u0026#39;) DEFAULT \u0026#39;normal\u0026#39;, status ENUM(\u0026#39;open\u0026#39;, \u0026#39;fulfilled\u0026#39;, \u0026#39;cancelled\u0026#39;) DEFAULT \u0026#39;open\u0026#39;, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ) `; await connection.execute(createUsersTable); await connection.execute(createDonationsTable); await connection.execute(createEmergencyRequestsTable); await connection.end(); return { statusCode: 200, body: JSON.stringify({ message: \u0026#39;Database initialized successfully\u0026#39; }) }; }; Verification Checklist RDS instance created and status is \u0026ldquo;Available\u0026rdquo; RDS is in Private Subnet (Public access: No) RDS Security Group only allows access from Lambda SG Endpoint and credentials saved securely Initial database daivietblood created Database schema initialized (tables created) Troubleshooting Issue Solution Cannot connect to RDS Verify Security Group allows inbound from Lambda SG RDS creation failed Check Service Quotas for RDS instances Connection timeout Ensure Lambda is in same VPC and has NAT Gateway access "},{"uri":"https://dokang307.github.io/Internship_Report/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/","title":"Create API Gateway","tags":[],"description":"","content":"Step 1: Create REST API Go to API Gateway Console ‚Üí Create API\nChoose API type:\nREST API ‚Üí Build Create new API:\nProtocol: REST Create new API: New API API name: daivietblood-api Description: REST API for DaiVietBlood system Endpoint Type: Regional Click Create API\nStep 2: Create Resources 2.1. Create /users Resource\nSelect root / ‚Üí Actions ‚Üí Create Resource\nConfigure:\nResource Name: users Resource Path: users Enable API Gateway CORS: ‚úÖ Check Click Create Resource\n2.2. Create /emergency-requests Resource\nSelect root / ‚Üí Actions ‚Üí Create Resource\nConfigure:\nResource Name: emergency-requests Resource Path: emergency-requests Enable API Gateway CORS: ‚úÖ Check Click Create Resource\nStep 3: Create Methods for /users 3.1. GET /users\nSelect /users ‚Üí Actions ‚Üí Create Method ‚Üí GET\nIntegration setup:\nIntegration type: Lambda Function Use Lambda Proxy integration: ‚úÖ Check Lambda Region: ap-southeast-1 Lambda Function: daivietblood-get-users Click Save ‚Üí OK (to add permission)\n3.2. POST /users\nSelect /users ‚Üí Actions ‚Üí Create Method ‚Üí POST\nIntegration setup:\nIntegration type: Lambda Function Use Lambda Proxy integration: ‚úÖ Check Lambda Region: ap-southeast-1 Lambda Function: daivietblood-create-user Click Save ‚Üí OK\nStep 4: Create Methods for /emergency-requests 4.1. GET /emergency-requests\nSelect /emergency-requests ‚Üí Actions ‚Üí Create Method ‚Üí GET\nIntegration setup:\nIntegration type: Lambda Function Use Lambda Proxy integration: ‚úÖ Check Lambda Function: daivietblood-emergency-requests Click Save ‚Üí OK\n4.2. POST /emergency-requests\nSelect /emergency-requests ‚Üí Actions ‚Üí Create Method ‚Üí POST\nIntegration setup:\nIntegration type: Lambda Function Use Lambda Proxy integration: ‚úÖ Check Lambda Function: daivietblood-emergency-requests Click Save ‚Üí OK\nStep 5: Enable CORS For each resource (/users, /emergency-requests):\nSelect resource ‚Üí Actions ‚Üí Enable CORS\nConfigure:\nAccess-Control-Allow-Methods: GET, POST, OPTIONS Access-Control-Allow-Headers: Content-Type, X-Amz-Date, Authorization, X-Api-Key Access-Control-Allow-Origin: * Click Enable CORS and replace existing CORS headers\nClick Yes, replace existing values\nStep 6: Deploy API Actions ‚Üí Deploy API\nDeployment stage:\nDeployment stage: [New Stage] Stage name: prod Stage description: Production stage Click Deploy\nCopy the Invoke URL:\nhttps://xxxxxxxxxx.execute-api.ap-southeast-1.amazonaws.com/prod ‚ÑπÔ∏è Save this URL. You will need it for frontend configuration.\nStep 7: API Structure Summary After completing, your API structure should look like:\ndaivietblood-api ‚îÇ ‚îú‚îÄ‚îÄ /users ‚îÇ ‚îú‚îÄ‚îÄ GET ‚Üí daivietblood-get-users ‚îÇ ‚îú‚îÄ‚îÄ POST ‚Üí daivietblood-create-user ‚îÇ ‚îî‚îÄ‚îÄ OPTIONS (CORS) ‚îÇ ‚îî‚îÄ‚îÄ /emergency-requests ‚îú‚îÄ‚îÄ GET ‚Üí daivietblood-emergency-requests ‚îú‚îÄ‚îÄ POST ‚Üí daivietblood-emergency-requests ‚îî‚îÄ‚îÄ OPTIONS (CORS) Verification Checklist REST API created /users resource created with GET and POST methods /emergency-requests resource created with GET and POST methods CORS enabled for all resources API deployed to prod stage Invoke URL saved "},{"uri":"https://dokang307.github.io/Internship_Report/5-workshop/5.2-prerequiste/","title":"Preparation","tags":[],"description":"","content":"Prerequisites Before starting this workshop, ensure you have:\n1. AWS Account\nActive AWS Account with Administrator access Recommended: Use IAM User instead of Root account Region: Asia Pacific (Singapore) - ap-southeast-1 2. Local Development Tools\nTool Version Purpose Node.js \u0026gt;= 18.x Run Lambda functions locally npm/yarn Latest Package management AWS CLI \u0026gt;= 2.x Interact with AWS services Git Latest Version control 3. Knowledge Requirements\nBasic understanding of AWS services (VPC, EC2, S3) Familiarity with REST APIs Basic Node.js/JavaScript or Python Basic React knowledge Step 1: Configure AWS CLI Install AWS CLI from AWS CLI Installation Guide\nConfigure credentials:\naws configure Enter your credentials: AWS Access Key ID: [Your Access Key] AWS Secret Access Key: [Your Secret Key] Default region name: ap-southeast-1 Default output format: json Verify configuration: aws sts get-caller-identity Step 2: Create IAM User for Workshop Go to IAM Console ‚Üí Users ‚Üí Create user\nUser details:\nUser name: workshop-admin Select: Provide user access to the AWS Management Console Set permissions:\nSelect: Attach policies directly Search and select: AdministratorAccess Create user and save credentials securely\n‚ö†Ô∏è Security Note: After completing the workshop, delete this IAM user or remove AdministratorAccess policy.\nStep 3: Verify Service Quotas Ensure your account has sufficient quotas for:\nService Resource Minimum Required VPC VPCs per Region 1 VPC Subnets per VPC 4 VPC NAT Gateways per AZ 1 RDS DB Instances 1 Lambda Concurrent Executions 10 API Gateway REST APIs 1 S3 Buckets 2 Check quotas at: Service Quotas Console ‚Üí Select service ‚Üí View quotas\nStep 4: Prepare Source Code Clone the sample repository: git clone https://github.com/your-repo/daivietblood-workshop.git cd daivietblood-workshop Project structure: daivietblood-workshop/ ‚îú‚îÄ‚îÄ frontend/ # React application ‚îÇ ‚îú‚îÄ‚îÄ src/ ‚îÇ ‚îî‚îÄ‚îÄ package.json ‚îú‚îÄ‚îÄ backend/ # Lambda functions ‚îÇ ‚îú‚îÄ‚îÄ functions/ ‚îÇ ‚îî‚îÄ‚îÄ package.json ‚îú‚îÄ‚îÄ infrastructure/ # CloudFormation templates ‚îÇ ‚îî‚îÄ‚îÄ templates/ ‚îî‚îÄ‚îÄ README.md Install dependencies: # Frontend cd frontend \u0026amp;\u0026amp; npm install # Backend cd ../backend \u0026amp;\u0026amp; npm install Step 5: Cost Estimation Service Configuration Est. Cost/Day NAT Gateway 1 NAT Gateway ~$1.08 RDS db.t3.micro ~$0.52 Lambda Free Tier $0.00 API Gateway Free Tier $0.00 S3 \u0026lt; 5GB ~$0.01 CloudFront \u0026lt; 1GB transfer ~$0.01 Amplify Build \u0026amp; Host ~$0.50 Total estimated: ~$2-3/day\nüí° Tip: Complete the workshop in 1-2 days and clean up resources immediately to minimize costs.\nChecklist Before Starting AWS Account ready with Administrator access AWS CLI installed and configured Node.js \u0026gt;= 18.x installed Git installed Source code cloned Region set to ap-southeast-1 "},{"uri":"https://dokang307.github.io/Internship_Report/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Dai Viet Blood Donation \u0026amp; Emergency System (DaiVietBlood) Implemented by: Skyline Team ‚Äì FPT University Ho Chi Minh City\nDate: December 7, 2025\nDownload PDF\nTABLE OF CONTENTS BACKGROUND AND MOTIVATION 1.1 Executive Summary 1.2 Project Success Criteria 1.3 Assumptions SOLUTION ARCHITECTURE / ARCHITECTURAL DIAGRAM 2.1 Technical Architecture Diagram 2.2 Technical Plan 2.3 Project Plan 2.4 Security Considerations ACTIVITIES AND DELIVERABLES 3.1 Activities and Deliverables 3.2 Out of Scope 3.3 Path to Production EXPECTED AWS COST BREAKDOWN IMPLEMENTATION TEAM RESOURCES \u0026amp; ESTIMATED PERSONNEL COSTS ACCEPTANCE 1. BACKGROUND AND MOTIVATION 1.1 EXECUTIVE SUMMARY Customer Background: The DaiVietBlood system is designed to serve the community, including voluntary blood donors, patients in need of emergency blood, and healthcare professionals in Vietnam. The primary customers are blood donors, patient families, and medical staff responsible for managing blood inventory and donation schedules. They require a centralized, reliable platform to optimize the matching process between donors and recipients and improve communication during emergencies. In the context of digital health transformation, DaiVietBlood provides a secure, accessible solution to address localized blood shortages.\nBusiness and Technical Objectives: Migrating the DaiVietBlood system from a local/on-premise environment to AWS offers superior advantages:\nBusiness: AWS allows the application to scale flexibly according to the user base, reduces hardware infrastructure operational costs, and ensures consistent performance nationwide. Technical: AWS provides High Availability and medical data security. Adopting a Serverless Architecture (AWS Lambda, API Gateway, Cognito, RDS) simplifies backend management, accelerates development, and reduces maintenance costs. The system integrates comprehensive monitoring (CloudWatch) and adheres to strict security standards. Summary of Key Use Cases:\nRole Key Function Short Description Guest Access Public Information View donation guidelines, blood compatibility charts, and educational articles without logging in. Member Register/Login, Profile Management Create accounts, update personal information and blood type. Book Blood Donation Select time slots and locations for donation. Submit Emergency Request Submit urgent blood requests; the system automatically finds suitable donors. Staff Manage Requests \u0026amp; Inventory Approve emergency requests, confirm donation schedules, update blood stock. Administrator (Admin) System Administration Manage user accounts, configure donation slots, view overview reports. Summary of Partner‚Äôs Professional Services: The Skyline Team will provide comprehensive digital transformation services, including assessing the current local application, designing a Cloud-native architecture, and executing the migration of the system to an AWS Serverless environment. We commit to delivering a secure, scalable system accompanied by automated CI/CD pipelines and detailed operational documentation.\n1.2 PROJECT SUCCESS CRITERIA Functionality: 100% of core functions (registration, scheduling, emergency requests, administration) operate stably on AWS with no regression errors. Availability: System achieves Uptime ‚â• 99.9%, ensuring continuous 24/7 access. Performance: Application response time improves by at least 30% compared to the local version. Emergency request processing time is reduced by 40%. Cost: Infrastructure costs are optimized by at least 20% thanks to the Serverless model and Auto-scaling. User Experience: UAT acceptance rate reaches a minimum of 95% for all user roles. Security: Full compliance with data encryption, access management (IAM), and API security requirements. Operations: CI/CD pipeline is fully automated with deployment time \u0026lt; 10 minutes. Monitoring system covers 100% of critical services. 1.3 ASSUMPTIONS Technical \u0026amp; Architectural Assumptions:\nSource Code: The current Local application (Frontend \u0026amp; Backend) is functionally complete. The project focuses on Refactoring for the Cloud (Serverless), excluding the development of new features. AWS Region: The entire infrastructure is deployed in Singapore (ap-southeast-1) to optimize latency for users in Vietnam. Note: During the testing phase, due to limited VPC configurations and Free Tier/Student resources, latency may fluctuate (estimated ~3.5s/request). Service Limits: The AWS account uses default limits (Soft limits). Increasing limits to reduce latency will be approved by the Customer when necessary. Third-party Integration: The system uses the Gemini API for AI support features. Access Rights: The Skyline Team is granted Admin access (IAM Role) to provision resources. Operational \u0026amp; Financial Assumptions:\nDomain: The Customer owns the domain name (e.g., daivietblood.com) and DNS configuration rights. Cost: The cost estimate is based on an assumption of approximately 50,000 API requests/month. Actual costs depend on usage levels (Pay-as-you-go). 2. SOLUTION ARCHITECTURE / ARCHITECTURAL DIAGRAM 2.1 TECHNICAL ARCHITECTURE DIAGRAM The DaiVietBlood system utilizes a Serverless-First architecture on AWS Cloud, prioritizing scalability, security, and operational optimization.\nKey Components:\nNetwork Infrastructure (VPC): Public Subnet: Contains Internet Gateway and NAT Gateway. Private Subnet: Contains AWS Lambda and Amazon RDS to isolate and secure data, preventing direct Internet access. Application \u0026amp; Data: Frontend: Hosted on AWS Amplify, distributed via Amazon CloudFront (CDN), and assets stored on S3. Authentication: Amazon Cognito manages identity and issues JWT tokens. API \u0026amp; Compute: Amazon API Gateway receives requests and routes them to AWS Lambda for business logic processing. Database: Amazon RDS stores structured data, located in the Private Subnet. DevOps \u0026amp; Monitoring: CI/CD: Uses AWS CodePipeline, CodeBuild, CodeDeploy to automate the deployment process. Monitoring: Amazon CloudWatch centrally collects logs and metrics. 2.2 TECHNICAL PLAN The technical implementation process follows the Infrastructure-as-Code (IaC) methodology:\nInfrastructure Automation: Use AWS CloudFormation to provision VPC, Lambda, RDS, and API Gateway, ensuring consistency across environments (Dev/Staging/Prod). Application Development: Refactor backend into modular Lambda functions (NodeJS/Python). Environment variables and sensitive information (DB credentials) are securely encrypted. CI/CD Process: Source (GitHub) -\u0026gt; Build (CodeBuild) -\u0026gt; Deploy (CloudFormation/CodeDeploy). Includes a Manual Approval step before deploying to the Production environment. Testing Strategy: Unit Tests for Lambda, Integration Tests for API, and Load Tests to ensure capacity. 2.3 PROJECT PLAN The project applies the Agile Scrum model over 8 weeks (4 Sprints):\nSprint 1 (Foundation): Set up AWS Account, VPC, RDS. Sprint 2 (Backend Core): Develop Lambda, API Gateway, Cognito. Sprint 3 (Integration): Deploy Frontend (Amplify), finalize CI/CD Pipeline. Sprint 4 (Stabilization): UAT, Performance Optimization, Handover. 2.4 SECURITY CONSIDERATIONS Access Management: Use Cognito for user authentication and IAM Roles for service authorization (Least Privilege). Network Isolation: Database and Lambda are located in the Private Subnet, accessing the Internet only via NAT Gateway. Data Protection: Data encryption At-rest (on RDS/S3) and In-transit (via HTTPS). Security Monitoring: CloudWatch Logs record all activities for auditing and intrusion detection. 3. ACTIVITIES AND DELIVERABLES 3.1 ACTIVITIES AND DELIVERABLES Phase Timeline Key Activities Deliverables Estimate (Man-days) Analysis \u0026amp; Design Week 1 Assess Local state, design Cloud architecture, plan migration. SRS Document, Architecture Diagram, API Specs. 5 Local Development Week 2-3 Build backend logic, database schema, local unit tests. Backend Prototype, Database Schema. 10 Frontend \u0026amp; Integration Week 4-5 Develop Frontend, integrate local APIs, prepare code for refactoring. Completed Local Application. 10 AWS Infrastructure Setup Week 6 Write CloudFormation scripts, provision VPC, RDS, IAM. IaC Templates, Secure VPC Environment. 5 Refactor \u0026amp; Deploy Backend Week 7-8 Convert to Lambda, configure API Gateway, Cognito. Serverless Backend active on AWS. 10 Deploy Frontend \u0026amp; CI/CD Week 9-10 Host Frontend on Amplify, set up automated Pipeline. Production URL, CI/CD Pipeline. 10 Testing \u0026amp; Go-live Week 11 UAT, Security Testing, Performance Optimization. UAT Report, Security Report. 5 Handover \u0026amp; Training Week 12 Transfer accounts, operations training, handover documentation. Operations Manual, Acceptance. 5 3.2 OUT OF SCOPE Due to time and resource limitations of the MVP phase, the following items are not included:\nOptimal user search algorithm based on real-time Geo-location (currently using simplified logic). Complex Auto-scaling for the Database layer (currently using basic RDS). Deep Latency Optimization for regions outside Singapore. Advanced Security Compliance standards such as HIPAA/PCI-DSS. 3.3 PATH TO PRODUCTION To upgrade from the current MVP to a large-scale Production system, the following are required:\nEnvironment Strategy: Strictly separate Dev/Staging/Prod environments across different AWS accounts (Multi-account strategy). Database Scaling: Migrate to Amazon Aurora Serverless or use Read Replicas to increase read/write capacity. Enhanced Monitoring: Integrate AWS X-Ray to trace requests and identify performance bottlenecks. Strengthened Security: Deploy AWS WAF with rules to block DDoS and automated bots; use Amazon Inspector for periodic vulnerability scanning. 4. EXPECTED AWS COST BREAKDOWN The following Cost Estimation is based on the Asia Pacific (Singapore) region, which is the standard region for low latency access from Vietnam.\nCategory Service Estimated Configuration Monthly Cost (USD) Network NAT Gateway 1 NAT Gateway (Required for Private Subnet) + Data Processing ~$43.13 VPC Subnets, Security Groups ~$13.14 CloudFront 5GB Data Transfer (Utilizing Free Tier) ~$3.00 Compute Lambda 1,000 requests, 512MB RAM (Free Tier) ~$0.00 API Gateway 1,000 requests ~$0.00 Database RDS db.t3.micro, 20GB Storage ~$21.74 Storage S3 5GB Storage, 200 requests ~$0.14 Hosting Amplify Build \u0026amp; Hosting, WAF enabled ~$16.77 Ops CloudWatch Logs, Metrics, Alarms ~$9.41 CI/CD CodePipeline 1 Active Pipeline ~$1.05 Total ~$108.38 / Month D∆∞·ªõi ƒë√¢y l√† n·ªôi dung m·ª•c 5. TEAM ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh d·∫°ng l·∫°i theo ƒë√∫ng c·∫•u tr√∫c b·∫£ng bi·ªÉu trong t√†i li·ªáu d·ª± √°n c·ªßa b·∫°n:\n5. TEAM AWS FCJ Program Lead (Mentor)\nName Title Description Email / Contact Info Nguy·ªÖn Gia H∆∞ng Head of Solutions Architect Provides technical mentorship, architecture review, and AWS best practices guidance. hunggia@amazon.com Project Stakeholders\nName Title Stakeholder for Email / Contact Info AWS FCJ Mentors Program Instructor Academic oversight, project evaluation, internship credit. [N/A] Internship Team (FPT University)\nName Title Role Email / Contact Info Nguy·ªÖn ƒê·ª©c L√¢n Co-Lead (PM) Project Manager: Responsible for team coordination, progress tracking, code structure, UAT, and Cost Optimization. lannguyen68609@gmail.com Nguy·ªÖn C√¥ng Minh Co-Lead (Technical) DevOps \u0026amp; Backend: CI/CD, CodePipeline, CDK Stack, Lambda implementation. minhncse182968@fpt.edu.vn ƒê·ªó Khang Member Cloud Architect: Architect Design, Service Policy, Co-designed AWS serverless architecture. dokhang307@gmail.com L√™ Ho√†ng Anh Member Fullstack Dev: API implementation, UI/UX design, Security configurations. anhlhse170327@fpt.edu.vn Nguy·ªÖn Qu√°ch Lam Giang Member Data Engineering: RDS with MySQL connection, VPC \u0026amp; Subnet configuration. nguyenlamgiang2198@gmail.com Project Escalation Contacts\nName Title Role Email / Contact Info Nguy·ªÖn ƒê·ª©c L√¢n Team Lead Primary contact for project status and escalations. lannguyen68609@gmail.com Here is the 6. RESOURCES \u0026amp; COST ESTIMATES section formatted professionally in both English and Vietnamese.\nI have adjusted the \u0026ldquo;Rate\u0026rdquo; column to reflect that this is an academic internship project (N/A or $0), and included a note explaining the cost efficiency.\n6. RESOURCES \u0026amp; COST ESTIMATES Resource Breakdown\nResource Responsibility Nguy·ªÖn ƒê·ª©c L√¢n Coordination \u0026amp; PM: Project Management, Cost Optimization, Forensic analysis, System configuration, and Full-stack development support. Nguy·ªÖn C√¥ng Minh DevOps \u0026amp; Infrastructure: CI/CD pipeline, Security, CDK Stack, Lambda implementation, Co-design API structure, System infrastructure. L√™ Ho√†ng Anh Frontend \u0026amp; UI/UX: Frontend development, API Integration, UI/UX Design, Application Security. Nguy·ªÖn Qu√°ch Lam Giang Data Engineering: Data Analysis, RDS to MySQL connection, VPC creation, CloudWatch monitoring, Subnet and NAT Gateway configuration. ƒê·ªó Khang Cloud Architecture: Architecture design, Service Policies, Documentation, API structure co-design, AI Chatbot integrations, CloudWatch monitoring. Hours by Project Phase\nProject Phase N.ƒê. L√¢n N.C. Minh L.H. Anh N.Q.L. Giang ƒê. Khang Total Hours Foundation 30 30 30 30 30 150 Core Orchestration 30 30 30 30 30 150 Analytics Layer 30 30 30 30 30 150 Testing \u0026amp; Validation 30 30 30 30 30 150 Documentation \u0026amp; Handover 30 30 30 30 30 150 Total Hours 150 150 150 150 150 750 Cost Distribution\nParty Contribution (USD) % Contribution of Total AWS FCJ Program $0 (Non-profit internship) 0% FPT University Student labor (Academic Credit) 0% AWS Infrastructure ~$15.00 (Testing \u0026amp; Running Costs) 100% Total Project Cost ~$15.00 100% *Note on Cost Efficiency: The labor cost for this project is subsidized as part of the FPT University Internship and AWS First Cloud Journey (FCJ) program. The estimated $15 represents the infrastructure running costs (AWS Credits) required for development and testing environments.\n7. ACCEPTANCE 7.1 Submission of Deliverables: Upon completion of the \u0026ldquo;Handover\u0026rdquo; phase, the Skyline Team will submit all Source Code, Architecture Documentation, Admin Accounts, and Operations Manuals to the Customer/Mentor.\n7.2 Acceptance Period \u0026amp; Process: The Customer has 05 business days to review and perform UAT. If the product meets the Success Criteria (Section 1.2), the Customer will sign the acceptance confirmation.\n7.3 Defect Remediation: If critical errors arise or features are missing compared to the committed scope, the Skyline Team is responsible for fixing and resubmitting for acceptance as soon as possible.\n"},{"uri":"https://dokang307.github.io/Internship_Report/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Goals: Mastering Compute Fundamentals: Deep dive into Amazon EC2 architecture, understanding the distinct use cases for different Instance Types. Network Security: Implementing robust firewall rules using Security Groups to control traffic flow. Cross-Platform Deployment: Hands-on practice deploying instances on both Linux and Windows environments using SSH and RDP. Application Setup: Configuring web server environments (LAMP/XAMPP) to simulate real-world application hosting. Tasks to be implemented this week: Day Task Start Date Completion Date Reference 2 Theory \u0026amp; Network Security:\n- Research EC2 core concepts (Instance Types, AMI, EBS, User Data).\n- Configure VPC Security Groups (Inbound/Outbound rules). 22/09/2025 22/09/2025 Module 02-Lab03-04.1 3 Basic Operations:\n- Practice launching EC2 instances in public subnets.\n- Establish connectivity via SSH key pairs. 22/09/2025 23/09/2025 Introduction to Amazon EC2 4 Linux Environment Deep Dive:\n- Deploy Amazon Linux EC2.\n- Advanced Lab: Recover access to an instance after losing the Key Pair.\n- Install LAMP Stack (Apache, MariaDB, PHP). 24/09/2025 24/09/2025 EC2 Linux Guide 5 Windows Environment Setup:\n- Deploy Windows Server instance.\n- Connect via RDP (Remote Desktop Protocol).\n- Install XAMPP solution. 25/09/2025 25/09/2025 EC2 Windows Guide Week 3 Achievement: Mastering EC2 Infrastructure, Security Configuration \u0026amp; Web Server Deployment 1. Deep Dive into EC2 Architecture \u0026amp; Components Compute Service Model: Gained a solid understanding of EC2 as an IaaS solution for on-demand scalable computing. Core Components: Analyzed the ecosystem required to run an instance effectively: Instance Types: Selection based on workload (CPU, Memory, Storage optimized). AMI (Amazon Machine Image): Using pre-configured templates for rapid deployment. Storage: Distinguished between EBS (persistent block storage) and Instance Store. Bootstrapping: Utilized User Data to run automated scripts upon instance initialization. 2. Network Security Implementation Security Group Management: Configured Security Groups as a stateful firewall at the instance level. Defined precise Inbound rules to allow necessary services (SSH port 22, HTTP port 80, RDP port 3389). Understood the default behavior of Outbound rules and connection tracking. 3. Linux Administration \u0026amp; Troubleshooting Connectivity: Mastered SSH connections using private keys (.pem). Disaster Recovery (Key Pair Loss): Successfully performed a manual recovery of a \u0026ldquo;lost\u0026rdquo; instance by: Creating a fresh Key Pair. Utilizing PuTTYgen to extract the Public Key. Injecting the new Public Key into the instance\u0026rsquo;s user-data script (cloud-config) while the instance is stopped. Validating access restoration after a reboot. Web Stack Deployment: Successfully installed and configured the LAMP stack (Linux, Apache, MariaDB, PHP) to host a test web page. 4. Windows Server Administration Access Management: Retrieved the Administrator password by decrypting it with the Key Pair file. Remote Access: Used Microsoft Remote Desktop (RDP/mstsc) for GUI-based management. Application Layer: Deployed XAMPP to verify web server functionality on a Windows environment. 5. Challenges \u0026amp; Solutions Issue 1 (Connectivity): SSH connection timed out initially. Solution: Audited the Security Group and added an Inbound Rule for port 22 from \u0026ldquo;My IP\u0026rdquo;. Issue 2 (Software Installation): Apache installation failed on Amazon Linux 2023 because the amazon-linux-extras command is deprecated. Solution: Researched documentation and switched to using dnf and specific distro-stream repositories compatible with AL2023. "},{"uri":"https://dokang307.github.io/Internship_Report/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/","title":"Test API Endpoints","tags":[],"description":"","content":"Step 1: Test from API Gateway Console 1.1. Test GET /users\nGo to API Gateway Console ‚Üí Select daivietblood-api Select /users ‚Üí GET Click Test Click Test button Expected response:\n{ \u0026#34;statusCode\u0026#34;: 200, \u0026#34;body\u0026#34;: \u0026#34;[]\u0026#34; } Step 2: Test with cURL Replace YOUR_API_URL with your actual Invoke URL.\n2.1. Create a User (POST /users)\ncurl -X POST https://YOUR_API_URL/prod/users \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;email\u0026#34;: \u0026#34;nguyen.van.a@example.com\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Nguyen Van A\u0026#34;, \u0026#34;blood_type\u0026#34;: \u0026#34;O+\u0026#34;, \u0026#34;phone\u0026#34;: \u0026#34;0901234567\u0026#34; }\u0026#39; Expected response:\n{ \u0026#34;id\u0026#34;: 1, \u0026#34;email\u0026#34;: \u0026#34;nguyen.van.a@example.com\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Nguyen Van A\u0026#34;, \u0026#34;blood_type\u0026#34;: \u0026#34;O+\u0026#34;, \u0026#34;phone\u0026#34;: \u0026#34;0901234567\u0026#34; } 2.2. Get All Users (GET /users)\ncurl https://YOUR_API_URL/prod/users Expected response:\n[ { \u0026#34;id\u0026#34;: 1, \u0026#34;email\u0026#34;: \u0026#34;nguyen.van.a@example.com\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Nguyen Van A\u0026#34;, \u0026#34;blood_type\u0026#34;: \u0026#34;O+\u0026#34;, \u0026#34;phone\u0026#34;: \u0026#34;0901234567\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2025-12-09T10:00:00.000Z\u0026#34; } ] 2.3. Create Emergency Request (POST /emergency-requests)\ncurl -X POST https://YOUR_API_URL/prod/emergency-requests \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;requester_name\u0026#34;: \u0026#34;Benh vien Cho Ray\u0026#34;, \u0026#34;blood_type\u0026#34;: \u0026#34;AB-\u0026#34;, \u0026#34;units_needed\u0026#34;: 5, \u0026#34;hospital\u0026#34;: \u0026#34;Cho Ray Hospital\u0026#34;, \u0026#34;urgency\u0026#34;: \u0026#34;critical\u0026#34; }\u0026#39; Expected response:\n{ \u0026#34;id\u0026#34;: 1, \u0026#34;message\u0026#34;: \u0026#34;Emergency request created\u0026#34; } 2.4. Get Emergency Requests (GET /emergency-requests)\ncurl https://YOUR_API_URL/prod/emergency-requests Step 3: Test with Postman Open Postman Create new Collection: DaiVietBlood API Add requests: Request Name Method URL Get Users GET {{baseUrl}}/users Create User POST {{baseUrl}}/users Get Emergency Requests GET {{baseUrl}}/emergency-requests Create Emergency Request POST {{baseUrl}}/emergency-requests Set Collection variable: baseUrl: https://YOUR_API_URL/prod Step 4: Verify Lambda Logs Go to CloudWatch Console ‚Üí Log groups\nFind log groups:\n/aws/lambda/daivietblood-get-users /aws/lambda/daivietblood-create-user /aws/lambda/daivietblood-emergency-requests Check recent log streams for:\nSuccessful invocations Any errors or exceptions Database connection logs Common Issues \u0026amp; Solutions Issue Cause Solution 502 Bad Gateway Lambda error Check CloudWatch logs for details Timeout Lambda cannot reach RDS Verify VPC, Subnets, Security Groups CORS error CORS not configured Enable CORS on API Gateway 500 Internal Server Error Database connection failed Check DB credentials in environment variables Step 5: Performance Check Note the response time for each API call First call may be slow (Lambda cold start) Subsequent calls should be faster Expected performance:\nEndpoint Cold Start Warm GET /users ~3-5s ~200-500ms POST /users ~3-5s ~200-500ms GET /emergency-requests ~3-5s ~200-500ms üí° Tip: Lambda cold start in VPC can be slow. Consider using Provisioned Concurrency for production workloads.\nVerification Checklist GET /users returns empty array or user list POST /users creates new user successfully GET /emergency-requests returns requests list POST /emergency-requests creates new request No CORS errors in browser console CloudWatch logs show successful invocations "},{"uri":"https://dokang307.github.io/Internship_Report/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Below is a list of blogs that have been translated into Vietnamese:\nBlog 1 - Optimizing SAP Operations with CloudWatch MCP server and Amazon Q CLI ‚Äì Part 3 This blog introduces how to combine Amazon CloudWatch MCP Server and Amazon Q CLI to optimize SAP operations. The article demonstrates using natural language to create SAP HANA system health reports, manage ASCS/ERS clusters, and simplify routine maintenance tasks. The solution aligns with the AWS Well-Architected Framework for SAP.\nBlog 2 - Streamlining SAP Operations with CloudWatch MCP Server and Amazon Q CLI ‚Äì Part 4 This blog continues from Part 3, illustrating advanced applications of CloudWatch MCP Server and Q CLI in SAP operations. The article guides you through orchestrating planned maintenance events, accelerating root cause analysis, and reducing Mean Time To Resolution (MTTR) for SAP systems.\nBlog 3 - Deploy Okta as a Custom Identity Provider for AWS Transfer Family This blog guides you through deploying Okta as a custom Identity Provider for AWS Transfer Family. The article demonstrates setting up MFA authentication with TOTP, configuring DynamoDB for user management, and testing SFTP connections with Okta authentication.\nBlog 4 - Getting Started with Healthcare Data Lakes: Using Microservices This blog introduces how to build a healthcare data lake using microservices architecture. The article covers pub/sub hub design, core microservice, front door microservice with Amazon Cognito, and staging ER7 microservice for processing HL7v2 data.\nBlog 5 - Getting Started with Healthcare Data Lakes: Using Microservices This blog introduces how to build a healthcare data lake using microservices architecture. The article covers pub/sub hub design, core microservice, front door microservice with Amazon Cognito, and staging ER7 microservice for processing HL7v2 data.\nBlog 6 - Getting Started with Healthcare Data Lakes: Using Microservices This blog introduces how to build a healthcare data lake using microservices architecture. The article covers pub/sub hub design, core microservice, front door microservice with Amazon Cognito, and staging ER7 microservice for processing HL7v2 data.\n"},{"uri":"https://dokang307.github.io/Internship_Report/5-workshop/5.3-s3-vpc/","title":"VPC &amp; Amazon RDS","tags":[],"description":"","content":"In this section, you will create the network infrastructure (VPC) and database (RDS) for the DaiVietBlood system.\nArchitecture Overview Content Create VPC Create Amazon RDS "},{"uri":"https://dokang307.github.io/Internship_Report/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Goals: Storage \u0026amp; Content Delivery: Understand and practice deploying static website hosting using Amazon S3 combined with the CloudFront content delivery network. Developer Tools (DevTools): Get familiar with using AWS CLI via VSCode to interact with AWS services more professionally and efficiently. Databases: Practice creating and managing relational databases with Amazon RDS (MySQL). Tasks to be implemented this week: Day Task Start Date Completion Date Reference 2 S3 \u0026amp; CloudFront:\n- Research core concepts of S3 and CloudFront.\n- Practice: Configure an S3 bucket for static website hosting, manage access permissions, create a CloudFront distribution, and practice Bucket Versioning. 29/09/2025 30/09/2025 Amazon S3 Guide 3 AWS CLI \u0026amp; VSCode:\n- Setup development environment: Connect AWS with VSCode.\n- Use AWS CLI to interact with services instead of using the Console. 30/09/2025 30/09/2025 AWS CLI Configuration 4 Databases:\n- Practice creating an RDS Instance.\n- Connect and work with MySQL on AWS infrastructure. 01/10/2025 01/10/2025 Amazon RDS (MySQL) Week 4 Achievement: Static Website, CDN \u0026amp; Database Administration 1. S3 Storage Mechanism and Static Website Hosting Understanding S3: Mastered S3 as an Object Storage service, identifying it as an ideal solution for hosting static web resources (HTML/CSS/JS). Deployment Process: Enabled the Static Website Hosting feature on the Bucket. Configured key documents: index.html (entry point) and error.html (error page). Access Management: Configured Bucket Policy to allow public read access or restricted access via CloudFront for enhanced security. Successfully tested website access via the S3 Endpoint URL. 2. Optimizing Content Delivery with CloudFront Distribution Configuration: Origin: Pointed to the S3 bucket containing the web source code. Behavior: Configured caching policies, Time-to-Live (TTL), and HTTP/HTTPS protocols. Key Benefits: Performance: Accelerated page load speeds leveraging the Edge Location network. Security: Hidden the origin S3 bucket from end-users (users interact only with CloudFront), supporting HTTPS via ACM Certificates. Operations: Understood how to use Invalidations to clear old cache when updating new website content. 3. Professional Interaction with AWS CLI \u0026amp; VSCode Environment Setup: Successfully installed the AWS Toolkit extension for VSCode. Configured user authentication (aws configure) using Access Key and Secret Key. Command Line Operations: Transitioned from using the graphical interface (Console) to using CLI commands for managing resources like S3, EC2, and RDS, significantly speeding up workflow. 4. Amazon RDS (MySQL) Administration Database Deployment: Successfully launched an RDS MySQL instance. Network Configuration: Set up Subnet Groups and Security Groups to control traffic flow to the database. Connection \u0026amp; Querying: Used MySQL Workbench to connect to the RDS endpoint. Performed basic database administration tasks (Creating DBs, executing SQL commands). "},{"uri":"https://dokang307.github.io/Internship_Report/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/","title":"Configure CORS &amp; Security","tags":[],"description":"","content":"Understanding CORS CORS (Cross-Origin Resource Sharing) is a security feature that restricts web pages from making requests to a different domain than the one serving the web page.\nWhen your React frontend (hosted on Amplify) calls your API Gateway, the browser checks CORS headers to determine if the request is allowed.\nStep 1: Configure CORS Headers in Lambda Ensure all Lambda functions return proper CORS headers:\nconst corsHeaders = { \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39;, // Or specific domain \u0026#39;Access-Control-Allow-Headers\u0026#39;: \u0026#39;Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token\u0026#39;, \u0026#39;Access-Control-Allow-Methods\u0026#39;: \u0026#39;GET,POST,PUT,DELETE,OPTIONS\u0026#39; }; // In your handler response: return { statusCode: 200, headers: corsHeaders, body: JSON.stringify(data) }; Step 2: Configure API Gateway CORS Method 1: Using Console\nGo to API Gateway Console ‚Üí Select your API For each resource: Select resource ‚Üí Actions ‚Üí Enable CORS Configure allowed origins, methods, headers Click Enable CORS and replace existing CORS headers Method 2: Using OPTIONS Method\nCreate OPTIONS method for each resource Integration type: Mock Add Method Response with status 200 Add Integration Response with headers: Access-Control-Allow-Headers: \u0026#39;Content-Type,X-Amz-Date,Authorization,X-Api-Key\u0026#39; Access-Control-Allow-Methods: \u0026#39;GET,POST,OPTIONS\u0026#39; Access-Control-Allow-Origin: \u0026#39;*\u0026#39; Step 3: API Gateway Security Best Practices 3.1. Enable API Key (Optional)\nGo to API Gateway ‚Üí API Keys ‚Üí Create API Key Name: daivietblood-api-key Go to Usage Plans ‚Üí Create Configure throttling and quota Associate API Key with Usage Plan For each method, set API Key Required: true 3.2. Enable Request Validation\nGo to API Gateway ‚Üí Models ‚Üí Create Create model for request body: { \u0026#34;$schema\u0026#34;: \u0026#34;http://json-schema.org/draft-04/schema#\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;CreateUserModel\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;required\u0026#34;: [\u0026#34;email\u0026#34;, \u0026#34;name\u0026#34;, \u0026#34;blood_type\u0026#34;], \u0026#34;properties\u0026#34;: { \u0026#34;email\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;email\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;minLength\u0026#34;: 1 }, \u0026#34;blood_type\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;enum\u0026#34;: [\u0026#34;A+\u0026#34;, \u0026#34;A-\u0026#34;, \u0026#34;B+\u0026#34;, \u0026#34;B-\u0026#34;, \u0026#34;AB+\u0026#34;, \u0026#34;AB-\u0026#34;, \u0026#34;O+\u0026#34;, \u0026#34;O-\u0026#34;] }, \u0026#34;phone\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } } } Apply model to POST method: Select method ‚Üí Method Request Request Validator: Validate body Request Body: Add model 3.3. Enable Throttling\nGo to Stages ‚Üí Select prod Stage Settings ‚Üí Default Method Throttling Configure: Rate: 100 requests/second Burst: 200 requests Step 4: Lambda Security Best Practices 4.1. Use AWS Secrets Manager for Credentials\nInstead of storing DB credentials in environment variables:\nGo to Secrets Manager ‚Üí Store a new secret\nSecret type: Other type of secret\nKey/value pairs:\nDB_HOST: daivietblood-db.xxxx.rds.amazonaws.com DB_USER: admin DB_PASSWORD: YourSecurePassword123! DB_NAME: daivietblood Secret name: daivietblood/db-credentials\nUpdate Lambda to retrieve secrets:\nconst { SecretsManagerClient, GetSecretValueCommand } = require(\u0026#39;@aws-sdk/client-secrets-manager\u0026#39;); const client = new SecretsManagerClient({ region: \u0026#39;ap-southeast-1\u0026#39; }); const getDbCredentials = async () =\u0026gt; { const command = new GetSecretValueCommand({ SecretId: \u0026#39;daivietblood/db-credentials\u0026#39; }); const response = await client.send(command); return JSON.parse(response.SecretString); }; Add IAM permission to Lambda role: { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:secretsmanager:ap-southeast-1:*:secret:daivietblood/*\u0026#34; } 4.2. Input Validation\nAlways validate input in Lambda:\nconst validateUser = (body) =\u0026gt; { const errors = []; if (!body.email || !isValidEmail(body.email)) { errors.push(\u0026#39;Invalid email\u0026#39;); } if (!body.name || body.name.length \u0026lt; 1) { errors.push(\u0026#39;Name is required\u0026#39;); } const validBloodTypes = [\u0026#39;A+\u0026#39;, \u0026#39;A-\u0026#39;, \u0026#39;B+\u0026#39;, \u0026#39;B-\u0026#39;, \u0026#39;AB+\u0026#39;, \u0026#39;AB-\u0026#39;, \u0026#39;O+\u0026#39;, \u0026#39;O-\u0026#39;]; if (!validBloodTypes.includes(body.blood_type)) { errors.push(\u0026#39;Invalid blood type\u0026#39;); } return errors; }; Step 5: Redeploy API After making changes:\nActions ‚Üí Deploy API Select prod stage Click Deploy Security Checklist CORS configured correctly Lambda returns proper CORS headers API Key enabled (optional but recommended) Request validation enabled Throttling configured DB credentials stored in Secrets Manager (recommended) Input validation in Lambda functions API redeployed after changes "},{"uri":"https://dokang307.github.io/Internship_Report/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":" In this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3‚Ä¶, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event‚Äôs content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Cloud Day Vietnam\nDate \u0026amp; Time: 09:00, September 18, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: Data Resiliency in a Cloud - First World\nDate \u0026amp; Time: 09:00, October 14, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: Data Science on AWS\nDate \u0026amp; Time: 09:00, October 16, 2025\nLocation: Hall B, FPT University Ho Chi Minh, District 9, Thu Duc, Ho Chi Minh\nRole: Attendee\nEvent 4 Event Name: AWS Cloud Mastery Series #1\nDate \u0026amp; Time: 09:00, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 5 Event Name: AWS Cloud Mastery Series #2\nDate \u0026amp; Time: 09:00, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 6 Event Name: AWS Cloud Mastery Series #3\nDate \u0026amp; Time: 09:00, November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://dokang307.github.io/Internship_Report/5-workshop/5.4-s3-onprem/","title":"Lambda &amp; API Gateway","tags":[],"description":"","content":"In this section, you will create AWS Lambda functions and expose them via Amazon API Gateway to build the serverless backend for DaiVietBlood.\nArchitecture Overview API Endpoints Method Endpoint Description GET /users Get all users POST /users Create new user GET /users/{id} Get user by ID GET /donations Get all donations POST /donations Create donation appointment GET /emergency-requests Get emergency requests POST /emergency-requests Create emergency request Content Create Lambda Functions Create API Gateway Test API Endpoints Configure CORS \u0026amp; Security "},{"uri":"https://dokang307.github.io/Internship_Report/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Goals: Tasks to be implemented this week: Day Task Start Date Completion Date Reference 2 - Study Module 5 theory: Security services on AWS\n- Shared Responsibility Model\n- Amazon IAM\n- AWS Cognito\n- AWS Organizations\n- AWS Identity Center\n- AWS KMS\n- AWS Security Hub 06/10/2025 06/10/2025 - IAM\n- Cognito\n- KMS 3 - Discuss and finalize group project\n- Services to be used\n- Data\n- Features 07/10/2025 07/10/2025 Meeting Minutes 4 - Research serverless services (features, pricing, \u0026hellip;) 08/10/2025 08/10/2025 Service 5 09/10/2025 09/10/2025 6 10/10/2025 10/10/2025 Week 5 achievement: Governance, Security, and Serverless Mindset AWS Security \u0026amp; Identity Management: Reinforced knowledge of the Shared Responsibility Model and identity management tools (Cognito, Identity Center). Mastered the roles of KMS and Security Hub in ensuring overall security posture. Serverless Architecture Research: Completed in-depth research on the Serverless model, focusing on Lambda, API Gateway, and DynamoDB. This is the foundation for the group project deployment. Capstone Project Initiation: Worked effectively as a team to finalize the architecture (using Serverless), assign tasks, and set up the initial development environment. The project will be an opportunity to apply all learned knowledge practically. Governance: Began understanding AWS Organizations and SCPs (Service Control Policies) as a means to manage and control resources across multiple accounts, preparing for larger scale. "},{"uri":"https://dokang307.github.io/Internship_Report/5-workshop/5.5-policy/","title":"S3, CloudFront &amp; Amplify","tags":[],"description":"","content":"In this section, you will set up Amazon S3 for static assets, CloudFront for content distribution, and AWS Amplify to host the React frontend application.\nArchitecture Overview Part 1: Amazon S3 Setup Step 1: Create S3 Bucket for Assets Go to S3 Console ‚Üí Create bucket\nGeneral configuration:\nBucket name: daivietblood-assets-{your-account-id} AWS Region: Asia Pacific (Singapore) ap-southeast-1 Object Ownership:\nACLs disabled (recommended) Block Public Access settings:\nBlock all public access: ‚úÖ (We\u0026rsquo;ll use CloudFront) Bucket Versioning:\nEnable (recommended for production) Default encryption:\nServer-side encryption: Enable Encryption type: Amazon S3 managed keys (SSE-S3) Click Create bucket\nStep 2: Create Bucket Policy for CloudFront After creating CloudFront distribution (Part 2), update bucket policy:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowCloudFrontAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;cloudfront.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::daivietblood-assets-{your-account-id}/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;AWS:SourceArn\u0026#34;: \u0026#34;arn:aws:cloudfront::{account-id}:distribution/{distribution-id}\u0026#34; } } } ] } Step 3: Upload Sample Assets Create folder structure:\n/images /blood-types /icons /banners /documents Upload sample images for the application\nPart 2: CloudFront Setup Step 1: Create CloudFront Distribution Go to CloudFront Console ‚Üí Create distribution\nOrigin settings:\nOrigin domain: Select your S3 bucket Origin path: Leave empty Name: daivietblood-s3-origin Origin access: Origin access control settings (recommended) Create new OAC: Click Create control setting Name: daivietblood-oac Signing behavior: Sign requests Default cache behavior:\nViewer protocol policy: Redirect HTTP to HTTPS Allowed HTTP methods: GET, HEAD Cache policy: CachingOptimized Settings:\nPrice class: Use only North America and Europe (or All edge locations) Default root object: index.html Click Create distribution\nImportant: Copy the bucket policy provided and update your S3 bucket policy\nStep 2: Get CloudFront Domain After distribution is deployed (takes 5-10 minutes):\nCopy the Distribution domain name:\nhttps://d1234567890.cloudfront.net Test accessing an asset:\nhttps://d1234567890.cloudfront.net/images/logo.png Part 3: AWS Amplify Setup Step 1: Prepare React Application Create React app (if not exists): npx create-react-app daivietblood-frontend cd daivietblood-frontend Install dependencies: npm install axios react-router-dom Create .env file: REACT_APP_API_URL=https://xxxxxxxxxx.execute-api.ap-southeast-1.amazonaws.com/prod REACT_APP_ASSETS_URL=https://d1234567890.cloudfront.net Sample API service (src/services/api.js): import axios from \u0026#39;axios\u0026#39;; const API_URL = process.env.REACT_APP_API_URL; export const getUsers = async () =\u0026gt; { const response = await axios.get(`${API_URL}/users`); return response.data; }; export const createUser = async (userData) =\u0026gt; { const response = await axios.post(`${API_URL}/users`, userData); return response.data; }; export const getEmergencyRequests = async () =\u0026gt; { const response = await axios.get(`${API_URL}/emergency-requests`); return response.data; }; export const createEmergencyRequest = async (requestData) =\u0026gt; { const response = await axios.post(`${API_URL}/emergency-requests`, requestData); return response.data; }; Push to GitHub repository Step 2: Deploy with Amplify Go to AWS Amplify Console ‚Üí Create new app\nChoose source:\nGitHub ‚Üí Continue Authorize AWS Amplify to access your GitHub Add repository branch:\nRepository: Select your repository Branch: main Configure build settings:\nApp name: daivietblood-frontend Build and test settings: Auto-detected for React Build settings (amplify.yml):\nversion: 1 frontend: phases: preBuild: commands: - npm ci build: commands: - npm run build artifacts: baseDirectory: build files: - \u0026#39;**/*\u0026#39; cache: paths: - node_modules/**/* Environment variables:\nAdd REACT_APP_API_URL and REACT_APP_ASSETS_URL Click Save and deploy\nStep 3: Configure Custom Domain (Optional) Go to App settings ‚Üí Domain management Click Add domain Enter your domain name Configure DNS records as instructed Part 4: Verify Deployment Access Amplify URL:\nhttps://main.d1234567890.amplifyapp.com Test functionality:\nHomepage loads correctly API calls work (check Network tab) Images load from CloudFront No CORS errors Verification Checklist S3 bucket created with proper settings CloudFront distribution deployed S3 bucket policy updated for CloudFront access Assets accessible via CloudFront URL React app deployed to Amplify Environment variables configured Frontend can call API Gateway Images load from CloudFront "},{"uri":"https://dokang307.github.io/Internship_Report/5-workshop/","title":"Workshop","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nBuilding Serverless System on AWS - DaiVietBlood Overview This workshop guides you through building a Serverless Blood Donation \u0026amp; Emergency System (DaiVietBlood) on AWS. You will learn how to set up and configure the core AWS services used in the project architecture.\nAWS Services Used Service Purpose Amazon VPC Create virtual private network with Public/Private Subnets NAT Gateway Allow resources in Private Subnet to access Internet Amazon RDS MySQL database for the application AWS Lambda Serverless business logic processing Amazon API Gateway Manage and expose REST APIs Amazon S3 Store static assets (images, files) Amazon CloudFront CDN for global content distribution AWS Amplify Host Frontend application (React) AWS CodePipeline CI/CD automation Amazon CloudWatch Monitoring and logging What You Will Learn Design and deploy Serverless-First architecture on AWS Configure VPC with Public/Private Subnets for security Create RDS MySQL in Private Subnet Build Lambda Functions and connect with API Gateway Store and distribute content with S3 and CloudFront Deploy React application with AWS Amplify Set up automated CI/CD Pipeline Monitor application with CloudWatch Prerequisites AWS Account with Administrator access Basic knowledge of AWS services Familiarity with Node.js and React AWS CLI installed and configured Estimated Cost This workshop uses resources within AWS Free Tier when possible. Estimated cost is approximately ~$15-20 if completed within 1-2 days and resources are cleaned up immediately after.\nContent Workshop Overview Preparation VPC \u0026amp; Amazon RDS Lambda \u0026amp; API Gateway S3, CloudFront \u0026amp; Amplify CI/CD, CloudWatch \u0026amp; Cleanup "},{"uri":"https://dokang307.github.io/Internship_Report/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Goals: Tasks to be implemented this week: Day Task Start Date Completion Date Reference 2 Start drawing AWS architecture diagrams 13/10/2025 13/10/2025 Architecture Diagram 3 Attend event \u0026ldquo;Data Resiliency in a Cloud-first World\u0026rdquo; 14/10/2025 14/10/2025 4 Write documentation for the proposal 15/10/2025 15/10/2025 Document Link 5 Attend event \u0026ldquo;Data Science on AWS\u0026rdquo; 16/10/2025 16/10/2025 6 17/10/2025 17/10/2025 Week 6 Harvest: Finalizing Solution Design and Data Orientation Project Architecture \u0026amp; Design: Transformed ideas into detailed AWS architecture diagrams. Completed the official Project Proposal, including technology decisions (Serverless) and non-functional requirements. Data Resiliency: Understood the importance of data resiliency following the data center incident in Korea. "},{"uri":"https://dokang307.github.io/Internship_Report/5-workshop/5.6-cleanup/","title":"CI/CD, CloudWatch &amp; Cleanup","tags":[],"description":"","content":"In this final section, you will set up CI/CD Pipeline, configure CloudWatch monitoring, and clean up all resources after completing the workshop.\nPart 1: CI/CD Pipeline with CodePipeline Step 1: Create CodeBuild Project Go to CodeBuild Console ‚Üí Create build project\nProject configuration:\nProject name: daivietblood-backend-build Description: Build project for Lambda functions Source:\nSource provider: GitHub Repository: Select your repository Branch: main Environment:\nEnvironment image: Managed image Operating system: Amazon Linux 2 Runtime: Standard Image: aws/codebuild/amazonlinux2-x86_64-standard:4.0 Service role: New service role Buildspec:\nBuild specifications: Use a buildspec file Create buildspec.yml file in your repository: version: 0.2 phases: install: runtime-versions: nodejs: 18 commands: - echo Installing dependencies... - cd backend \u0026amp;\u0026amp; npm ci pre_build: commands: - echo Running tests... - npm test || true build: commands: - echo Building Lambda packages... - mkdir -p dist - zip -r dist/get-users.zip functions/get-users/ - zip -r dist/create-user.zip functions/create-user/ - zip -r dist/emergency-requests.zip functions/emergency-requests/ post_build: commands: - echo Updating Lambda functions... - aws lambda update-function-code --function-name daivietblood-get-users --zip-file fileb://dist/get-users.zip - aws lambda update-function-code --function-name daivietblood-create-user --zip-file fileb://dist/create-user.zip - aws lambda update-function-code --function-name daivietblood-emergency-requests --zip-file fileb://dist/emergency-requests.zip artifacts: files: - dist/**/* Click Create build project Step 2: Create CodePipeline Go to CodePipeline Console ‚Üí Create pipeline\nPipeline settings:\nPipeline name: daivietblood-pipeline Service role: New service role Source stage:\nSource provider: GitHub (Version 2) Connection: Create new connection or select existing Repository name: Select your repository Branch name: main Output artifact format: CodePipeline default Build stage:\nBuild provider: AWS CodeBuild Project name: daivietblood-backend-build Deploy stage:\nSkip deploy stage (Lambda is updated in build stage) Click Create pipeline\nStep 3: Add IAM Permissions for CodeBuild Go to IAM Console ‚Üí Roles Find role codebuild-daivietblood-backend-build-service-role Add inline policy: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;lambda:UpdateFunctionCode\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:lambda:ap-southeast-1:*:function:daivietblood-*\u0026#34; } ] } Part 2: CloudWatch Monitoring Step 1: Create CloudWatch Dashboard Go to CloudWatch Console ‚Üí Dashboards ‚Üí Create dashboard\nDashboard name: DaiVietBlood-Monitoring\nAdd widgets:\nWidget 1: Lambda Invocations\nWidget type: Line Metrics: Lambda ‚Üí By Function Name ‚Üí Invocations Select all daivietblood functions Widget 2: Lambda Errors\nWidget type: Number Metrics: Lambda ‚Üí By Function Name ‚Üí Errors Statistic: Sum Widget 3: Lambda Duration\nWidget type: Line Metrics: Lambda ‚Üí By Function Name ‚Üí Duration Statistic: Average Widget 4: API Gateway Requests\nWidget type: Line Metrics: ApiGateway ‚Üí By Api Name ‚Üí Count Widget 5: RDS Connections\nWidget type: Line Metrics: RDS ‚Üí Per-Database Metrics ‚Üí DatabaseConnections Step 2: Create CloudWatch Alarms Alarm 1: Lambda Errors\nGo to CloudWatch ‚Üí Alarms ‚Üí Create alarm Select metric: Lambda ‚Üí By Function Name ‚Üí Errors Conditions: Threshold type: Static Whenever Errors is: Greater than 5 Period: 5 minutes Notification: Create new SNS topic: daivietblood-alerts Email: your-email@example.com Alarm name: DaiVietBlood-Lambda-Errors Alarm 2: RDS CPU High\nCreate alarm Select metric: RDS ‚Üí Per-Database Metrics ‚Üí CPUUtilization Conditions: Threshold: Greater than 80% Period: 5 minutes Notification: Use existing SNS topic Alarm name: DaiVietBlood-RDS-CPU-High Alarm 3: API Gateway 5XX Errors\nCreate alarm Select metric: ApiGateway ‚Üí By Api Name ‚Üí 5XXError Conditions: Threshold: Greater than 10 Period: 5 minutes Alarm name: DaiVietBlood-API-5XX-Errors Step 3: Configure Log Insights Go to CloudWatch ‚Üí Logs ‚Üí Logs Insights\nSelect log groups:\n/aws/lambda/daivietblood-get-users /aws/lambda/daivietblood-create-user /aws/lambda/daivietblood-emergency-requests Sample query - Find errors:\nfields @timestamp, @message | filter @message like /ERROR/ | sort @timestamp desc | limit 50 Sample query - Duration statistics: fields @timestamp, @duration | stats avg(@duration), max(@duration), min(@duration) by bin(1h) Part 3: Resource Cleanup ‚ö†Ô∏è Important: Follow these steps to avoid unexpected charges.\nCleanup Order (Important!) Clean up in the following order to avoid dependency errors:\nStep 1: Delete Amplify App Go to Amplify Console Select daivietblood-frontend Actions ‚Üí Delete app Confirm deletion Step 2: Delete CloudFront Distribution Go to CloudFront Console Select distribution ‚Üí Disable Wait for status to change to \u0026ldquo;Deployed\u0026rdquo; Select distribution ‚Üí Delete Step 3: Delete S3 Buckets Go to S3 Console Select bucket daivietblood-assets-* Empty bucket first Then Delete bucket Step 4: Delete API Gateway Go to API Gateway Console Select daivietblood-api Actions ‚Üí Delete Step 5: Delete Lambda Functions Go to Lambda Console Delete each function: daivietblood-get-users daivietblood-create-user daivietblood-emergency-requests Delete Lambda Layer: mysql2-layer Step 6: Delete RDS Instance Go to RDS Console ‚Üí Databases Select daivietblood-db Actions ‚Üí Delete Uncheck \u0026ldquo;Create final snapshot\u0026rdquo; Check \u0026ldquo;I acknowledge\u0026hellip;\u0026rdquo; Type delete me to confirm Step 7: Delete VPC Resources Go to VPC Console\nDelete NAT Gateway:\nNAT Gateways ‚Üí Select NAT Gateway ‚Üí Delete Wait for status \u0026ldquo;Deleted\u0026rdquo; Release Elastic IP:\nElastic IPs ‚Üí Select EIP ‚Üí Release Delete VPC Endpoints (if any):\nEndpoints ‚Üí Select endpoints ‚Üí Delete Delete Security Groups (except default):\nSecurity Groups ‚Üí Delete daivietblood-lambda-sg, daivietblood-rds-sg Delete DB Subnet Group:\nRDS Console ‚Üí Subnet groups ‚Üí Delete daivietblood-db-subnet-group Delete VPC:\nYour VPCs ‚Üí Select daivietblood-vpc ‚Üí Delete VPC This will delete subnets, route tables, internet gateway Step 8: Delete CI/CD Resources CodePipeline Console ‚Üí Delete daivietblood-pipeline CodeBuild Console ‚Üí Delete daivietblood-backend-build Step 9: Delete CloudWatch Resources CloudWatch ‚Üí Dashboards ‚Üí Delete DaiVietBlood-Monitoring CloudWatch ‚Üí Alarms ‚Üí Delete all related alarms CloudWatch ‚Üí Log groups ‚Üí Delete log groups /aws/lambda/daivietblood-* Step 10: Delete IAM Resources IAM Console ‚Üí Roles Delete roles: daivietblood-lambda-role codebuild-daivietblood-* codepipeline-daivietblood-* Cleanup Checklist Amplify app deleted CloudFront distribution deleted S3 buckets emptied and deleted API Gateway deleted Lambda functions and layers deleted RDS instance deleted NAT Gateway deleted Elastic IP released VPC and all components deleted CodePipeline and CodeBuild deleted CloudWatch dashboards, alarms, log groups deleted IAM roles deleted Verify No Remaining Charges Go to AWS Cost Explorer Verify no resources are running Go to Billing Console ‚Üí Bills to confirm üí° Tip: Set up Budget Alert in AWS Budgets to receive notifications when costs exceed threshold.\nWorkshop Conclusion Congratulations! üéâ You have completed the workshop on building a Serverless system on AWS.\nWhat You Learned: ‚úÖ Design and deploy VPC with Public/Private Subnets ‚úÖ Create RDS MySQL in a secure environment ‚úÖ Build Lambda functions and expose via API Gateway ‚úÖ Configure S3 and CloudFront for static assets ‚úÖ Deploy React app with AWS Amplify ‚úÖ Set up automated CI/CD Pipeline ‚úÖ Monitor application with CloudWatch Next Steps: Learn more about AWS Well-Architected Framework Explore advanced features like X-Ray tracing Experiment with Aurora Serverless for database Implement authentication with Amazon Cognito "},{"uri":"https://dokang307.github.io/Internship_Report/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at Amazon Web Services Vietnam Company Limited from September 8, 2025 to December 28, 2025, I had the opportunity to learn, hone my skills, and apply the academic knowledge I acquired at university to a real-world working environment.\nI participated in 8 office sessions and 6 events. Although I personally feel that the number of activities I attended was relatively limited, these sessions provided me with the opportunity to network with new peers and learn a great deal from them. Additionally, the office visits allowed me to connect with the mentors. I had several interactions with them, but I built the closest connection with Mr. Thinh and Mr. Hoang Anh, with whom I frequently exchange messages.\nThrough these experiences, I have noticed a gradual shift in my mindset. I am transitioning from a student perspective to becoming more mature and adapting to the requirements of corporate culture. I believe this transformation is valuable and essential to prepare myself for my future career.\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ‚úÖ ‚òê ‚òê 2 Ability to learn Ability to absorb new knowledge and learn quickly ‚òê ‚úÖ ‚òê 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ‚úÖ ‚òê ‚òê 4 Sense of responsibility Completing tasks on time and ensuring quality ‚úÖ ‚òê ‚òê 5 Discipline Adhering to schedules, rules, and work processes ‚òê ‚òê ‚úÖ 6 Progressive mindset Willingness to receive feedback and improve oneself ‚òê ‚úÖ ‚òê 7 Communication Presenting ideas and reporting work clearly ‚òê ‚úÖ ‚òê 8 Teamwork Working effectively with colleagues and participating in teams ‚úÖ ‚òê ‚òê 9 Professional conduct Respecting colleagues, partners, and the work environment ‚úÖ ‚òê ‚òê 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ‚òê ‚úÖ ‚òê 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ‚úÖ ‚òê ‚òê 12 Overall General evaluation of the entire internship period ‚úÖ ‚òê ‚òê Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "},{"uri":"https://dokang307.github.io/Internship_Report/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Goals: Systematize all knowledge acquired in the first half of the program. Deep dive into two main pillars: Security and Resiliency. Prepare a solid foundation for the Midterm Exam. Tasks to be implemented this week: Day Task Start Date Completion Date Reference 2 Midterm Review:\nPart 1: Secure Architecture\n- IAM (Identity \u0026amp; Access Management)\n- KMS (Key Management Service)\n- Security Group \u0026amp; NACLs\n- Secrets Manager\n- GuardDuty (Threat Detection)\n- Shield \u0026amp; WAF (DDoS \u0026amp; Web App Protection)\nPart 2: Resilient Architecture\n- Core metrics to memorize (RTO/RPO)\n- Multi-AZ RDS\n- Disaster Recovery Strategies\n- Auto Scaling and Load Balancing\n- Route 53 and DNS\n- AWS Backup 20/10/2025 20/10/2025 Review Link 3 Self-study: Systematize security services and practice simulation labs 21/10/2025 21/10/2025 4 Self-study: Redraw High Availability (HA) and Disaster Recovery (DR) architecture diagrams 22/10/2025 22/10/2025 5 Take Practice Tests to review knowledge 23/10/2025 23/10/2025 practice 6 Compile questions and review key notes 24/10/2025 24/10/2025 Tips Week 7 achievement: Consolidating Security Mindset and Resilient Architecture Defense in Depth Mindset:\nClearly distinguished the difference and scope between Security Groups (Stateful - Instance level) and NACLs (Stateless - Subnet level). Deeply understood protection at different layers: WAF for applications (Layer 7), Shield for infrastructure (Layer 3/4), and GuardDuty for intelligent threat detection. Mastered the importance of KMS and Secrets Manager in protecting sensitive data \u0026ldquo;at rest\u0026rdquo; and \u0026ldquo;in transit\u0026rdquo;. Resiliency \u0026amp; Recovery:\nMemorized and understood the nature of RTO (Recovery Time Objective) and RPO (Recovery Point Objective) metrics to select appropriate DR strategies (Backup \u0026amp; Restore, Pilot Light, Warm Standby, or Multi-site Active/Active). Mastered Multi-AZ RDS mechanisms for High Availability compared to Read Replicas (used for scaling reads). Understood the coordination between Route 53, ELB, and Auto Scaling to create a flexible, self-healing system capable of handling load effectively. "},{"uri":"https://dokang307.github.io/Internship_Report/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" Here, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don‚Äôt understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n"},{"uri":"https://dokang307.github.io/Internship_Report/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Goals: Consolidate knowledge acquired from previous weeks. Review key architectural patterns (High Performance \u0026amp; Cost Optimization). Complete the Midterm Examination. Tasks to be implemented this week: Day Task Start Date Completion Date Reference 2 Midterm Review:\n1. Part 3: High Performance Architecture:\n- AWS Fargate - Serverless Container\n- AWS Lambda - Event-Driven Architecture\n- Auto Scaling with CloudWatch\n2. Part 4: Cost-optimized Architecture\n- AWS Cost Explorer - Cost Analysis \u0026amp; Optimization\n- S3 Storage Tiering - Lifecycle Automated Cost Optimization\n- Complete VPC Architecture with NAT Gateway 27/10/2025 27/10/2025 Review Link 3 Self-study and consolidation of knowledge 28/10/2025 28/10/2025 4 Self-study and practice tests 29/10/2025 29/10/2025 Practice 5 Final review preparation 30/10/2025 30/10/2025 6 Midterm Exam 31/10/2025 31/10/2025 Week 8 achievement: Consolidating High-Performance \u0026amp; Cost-Optimized Architectures High-Performance Architecture Mastery:\nDeepened understanding of Serverless Containers (AWS Fargate) vs. Event-Driven Compute (AWS Lambda), and when to apply each for optimal performance. Reinforced the mechanics of Auto Scaling triggered by CloudWatch metrics to ensure system stability under varying loads. Cost Optimization Strategies:\nMastered the use of AWS Cost Explorer to analyze spending patterns. Reviewed S3 Storage Tiering and Lifecycle policies to automate cost savings for data storage. Consolidated understanding of networking costs, specifically focusing on NAT Gateways within a complete VPC architecture. Knowledge Validation:\nSuccessfully completed the Midterm Exam, validating the ability to design architectures that balance performance, security, and cost-effectiveness. The exam highlighted areas of strength in networking and areas for further refinement in serverless orchestration. "},{"uri":"https://dokang307.github.io/Internship_Report/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Goals: Build and train the Recommendation Model. Refine data and handle issues arising during the training process. Tasks to be implemented this week: Day Task Start Date Completion Date Reference 2 Model Building:\n- Configure model recipe\n- Create dataset import job\n- Run training process 10/11/2025 11/11/2025 AWS Personalize Docs 3 Preliminary model check and error analysis 12/11/2025 12/11/2025 4 Data Refinement \u0026amp; Update:\n- Add new fields to the Schema\n- Clean and resynchronize the dataset\n- Retrain model with updated data 12/11/2025 13/11/2025 5 Re-evaluate metrics after data update 14/11/2025 14/11/2025 6 Week 9 Harvest: Challenges in Model Training and Data Quality This week focused heavily on model training techniques, and I faced real-world Data Science challenges:\nData Scarcity \u0026amp; Interaction Limitations:\nIssue: The dataset volume is limited, and critically, there is only one interaction type: \u0026ldquo;Booking\u0026rdquo;. The lack of high-funnel interactions like \u0026ldquo;View\u0026rdquo; or \u0026ldquo;Click\u0026rdquo; makes it difficult for the model to capture latent user preferences. Consequence: The model\u0026rsquo;s evaluation metrics after training are relatively low, leading to significant skepticism regarding the accuracy and practical effectiveness of the recommendations. Validation Blockers:\nSince the Website Frontend is not yet complete, I cannot perform real-world validation (visual checks) to see if the recommendations make sense intuitively. Currently, I rely solely on abstract metrics. Data Schema Evolution:\nThe addition of new data fields during development caused friction in the data processing pipeline. Every schema change required re-cleaning the data, re-mapping the schema, and retraining the model from scratch, which was time-consuming. "},{"uri":"https://dokang307.github.io/Internship_Report/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Solution Architecture: Finalize High-Level (HLD) and Low-Level (LLD) architecture diagrams based on the AWS Well-Architected Framework. Resource Planning \u0026amp; Costing: Estimate Total Cost of Ownership (TCO) and select appropriate services. Network Infrastructure Initialization (Networking): Establish the foundation for VPC, Subnets, and basic security policies. Tasks to be implemented this week: Day Task Start Date Completion Date Reference 2-3 Design \u0026amp; Planning:\n- Analyze project business requirements.\n- Draw system architecture diagram.\n- Use AWS Pricing Calculator to create a cost estimate.\n- Review architecture against the 5 pillars of the Well-Architected Framework. 10/11/2025 11/11/2025 4-5 Deploy Network Infrastructure (IaC):\n- Write code (Terraform/CloudFormation) to initialize VPC, Public/Private Subnets, NAT Gateway, Route Tables.\n- Design and configure basic Security Groups for Bastion Host, Web Server, and Database. 12/11/2025 13/11/2025 Week 10 Achievements: Architecture Diagrams \u0026amp; Infrastructure Foundation 1. System Architecture Design Architecture Diagram: Completed data flow diagram and resource layout. The system is designed following the Single-AZ model. Service Selection: Database: RDS MySQL (Multi-AZ) for relational data. Storage: S3 for static assets and backups. Cost Optimization: Created a monthly budget estimate and identified saving strategies (such as using Spot Instances for Dev environments, Savings Plans for Prod). 2. Network Deployment (VPC) Network Structure: Successfully established VPC with custom CIDR block, clearly separating Public Subnets and Private Subnets (for App, DB). Network Security: Established Security Groups following the principle of Least Privilege. "},{"uri":"https://dokang307.github.io/Internship_Report/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Deploy Core Services: Configure Compute and Database resources. Automation (DevOps): Build a CI/CD pipeline to automate code building and deployment to the AWS environment. Tasks to be implemented this week: Day Task Start Date Completion Date Reference 2-3 Compute \u0026amp; DB Configuration:\n- Initialize RDS with Single-AZ.\n- Connect to MySQL. 17/11/2025 18/11/2025 4-5 Build CI/CD Pipeline:\n- Setup Repo on CodeCommit/GitHub.\n- Configure CodeBuild to package the application.\n- Use CodeDeploy/CodePipeline to automatically deploy code to EC2 Instances.\n- Verify the code deployment flow from Local to Cloud. 19/11/2025 20/11/2025 Week 11 Achievements: Operational System \u0026amp; Automated Pipeline 1. High Availability Application Deployment Database: RDS is operating stably in Multi-AZ mode, ensuring data safety in case of failure in one AZ. Successfully connected the App Server to the DB. 2. DevOps Process (CI/CD) Complete Pipeline: Successfully built a CI/CD (Continuous Integration/Continuous Deployment) flow. Source: Code is pushed to GitHub/CodeCommit. Build: Automatically install dependencies and package artifacts. Deploy: Automatically update the new version to all EC2 instances in the ASG without service interruption (Zero Downtime Deployment - using Rolling Update strategy). "},{"uri":"https://dokang307.github.io/Internship_Report/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Security \u0026amp; Monitoring: Scan for security vulnerabilities and set up a system monitoring Dashboard. Project Wrap-up: Finalize documentation, perform final optimizations, and prepare presentation slides. Tasks to be implemented this week: Day Task Start Date Completion Date Reference 2-3 - Configure CloudWatch Alarms and Dashboard.\n- Perform security review using AWS Trusted Advisor and GuardDuty. 24/11/2025 25/11/2025 4-5 Optimization \u0026amp; Packaging:\n- Review and delete redundant resources for cost optimization.\n- Write operational documentation (Runbook).\n- Draft final report slides (Architecture, Challenges, Lessons Learned). 26/11/2025 27/11/2025 Week 12 Achievements: System Finalization \u0026amp; Handover Readiness 1. Performance Evaluation \u0026amp; Tuning Monitoring: Established a CloudWatch Dashboard to visually display key metrics: CPU Utilization, Request Count, Database Connections, Error Rate (4xx, 5xx). Configured SNS Alerts to send email notifications to Admins when system issues occur. 2. Optimization \u0026amp; Advanced Security Security: Enabled WAF (Web Application Firewall) to block common attacks (SQL Injection, XSS). Reviewed IAM Roles and revoked unnecessary privileges. Cost: Based on data from the 2-week pilot run, resized Instances from t3.medium to t3.micro for the Dev environment to save costs without impacting performance. 3. Project Completion Documentation: Completed the project documentation suite including: Updated Architecture Diagram, Redeployment Guide, and Cost Report. Lessons Learned: Gained insights into handling \u0026ldquo;Cold Start\u0026rdquo; issues during Auto Scaling and the importance of accurate Health Check configuration. "},{"uri":"https://dokang307.github.io/Internship_Report/4-eventparticipated/4.6-event6/","title":"","tags":[],"description":"","content":"Here is the English version of the report, maintaining the structure and formatting suitable for your AWS program submission.\ntitle: \u0026ldquo;Event 6\u0026rdquo; date: \u0026ldquo;2025-09-08\u0026rdquo; weight: 1 chapter: false pre: \u0026quot; 4.6. \u0026quot;\nEvent Summary Report: AWS Cloud Mastery Series #3 Topic: AWS Well-Architected ‚Äì Security Pillar Workshop\n1. Overview \u0026amp; Speakers The event focused on the most critical pillar within the AWS Well-Architected Framework: Security. The content was designed to equip attendees with knowledge ranging from fundamental to advanced levels regarding identity, monitoring, infrastructure protection, data protection, and incident response processes.\nSpeakers \u0026amp; Experts: The event gathered experts from the AWS Community (AWS Community Builders), AWS Cloud Club Captains from various universities (HCMUTE, SGU, PTIT, HUFLIT), Cloud Engineers from FCJ, and specially featuring Mendel Grabski (Security \u0026amp; DevOps Expert).\nAbout AWS Cloud Club: This is a network connecting students and professionals, helping to develop technical leadership skills, providing hands-on experiences, and offering long-term mentoring opportunities. The participating Cloud Clubs under FCJA include: HCMUTE, SGU, PTIT, and HUFLIT.\n2. Key Technical Highlights A. Identity \u0026amp; Access Management (IAM) IAM is defined as the \u0026ldquo;first line of defense.\u0026rdquo; The session emphasized the shift from manual management to automation and strict adherence to key principles:\nLeast Privilege Principle: Grant only the necessary permissions required to perform a task. Root User Protection: Delete access keys immediately after creation. Service Control Policies (SCPs): Use Organization-level policies to set a \u0026ldquo;ceiling\u0026rdquo; (maximum available permissions) for member accounts (Note: SCPs only filter permissions; they do not grant them). Permission Boundaries: Set the maximum permissions that an identity-based policy can grant to a specific User/Role. Multi-Factor Authentication (MFA): Encouraged the use of FIDO2 (hardware keys/biometrics) over traditional TOTP. Credential Rotation: Use AWS Secrets Manager to automate the rotation process (create -\u0026gt; set -\u0026gt; test -\u0026gt; finish) and integrate with EventBridge to manage schedules, eliminating risks associated with \u0026ldquo;hardcoded credentials.\u0026rdquo; B. Continuous Monitoring \u0026amp; Threat Detection The focus was on building comprehensive visibility and automated response capabilities:\nMulti-Layer Monitoring: Combining CloudTrail (recording API calls, S3/Lambda access) and VPC Flow Logs (network traffic). Event-Driven Security: Using EventBridge as a central event bus to route real-time alerts to Lambda/SNS/SQS or coordinate actions across different accounts (Cross-account routing). Detection-as-Code: Managing detection rules and queries (CloudTrail Lake queries) as code (version control) to ensure consistent deployment across the organization. Deep Dive into Amazon GuardDuty: This is an intelligent threat detection solution that operates continuously based on three main data sources: CloudTrail, VPC Flow Logs, and DNS Logs.\nExpanded Coverage: Protection for S3, EKS, RDS (brute-force detection), Lambda (suspicious network activity), and Malware Protection (EBS scanning). Runtime Monitoring: Uses an Agent to monitor deep inside the OS (processes, file access) on EC2/EKS/Fargate. C. Compliance \u0026amp; Infrastructure as Code (IaC) Security compliance is no longer a manual check but is integrated into the deployment pipeline:\nApplied Standards: AWS Foundational Security Best Practices, CIS Benchmark, PCI DSS, NIST. Enforcement Mechanism: Using AWS CloudFormation (IaC) to deploy standard configurations, combined with AWS Security Hub to automatically audit resources against defined standards. D. Network \u0026amp; Data Protection Network Security: Clearly distinguishing between Security Groups (Stateful - instance level firewall) and NACLs (Stateless - subnet level firewall). Introduction to AWS Network Firewall for advanced Egress filtering/IPS and integration with Threat Intelligence to automatically block malicious traffic. Data Protection: Encryption: Using KMS with Customer Master Keys (CMK) and Policy conditions to control decryption contexts. Certificates: Using AWS ACM to manage and automatically renew SSL/TLS certificates. Service Security: Enforcing HTTPS/TLS 1.2+ for S3 (via Bucket Policy) and Databases (e.g., PostgreSQL rds.force_ssl=1). E. Incident Response (IR) The standard IR process consists of 5 steps: Preparation -\u0026gt; Detection \u0026amp; Analysis -\u0026gt; Containment -\u0026gt; Eradication \u0026amp; Recovery -\u0026gt; Post-Incident Activity.\nPrevention Strategy: Never make S3 buckets public, isolate sensitive services in private subnets, and ensure all infrastructure changes go through IaC with a review process (double-gate). 3. Practical Experience \u0026amp; Q\u0026amp;A The event provided high practical value, specifically aligning with the \u0026ldquo;Automated Incident Response and Forensics\u0026rdquo; project our team is developing.\nDiscussion Point: During our project testing, the team noticed that Amazon GuardDuty has a latency of about 5 minutes to generate a finding after an incident occurs. We asked about solutions to reduce this latency.\nExpert Answer:\nThe Nature of the Service: The GuardDuty latency is an accepted technical characteristic because the system needs time to analyze large datasets to accurately determine threats and avoid false positives. Alternative Solutions: To achieve near real-time detection, the expert suggested integrating 3rd party solutions like OpenClarity (open source) or building custom anomaly detection logic based on CloudTrail events. Networking: After the event, Mr. Mendel Grabski (Ex-Head of Security \u0026amp; DevOps) expressed interest and offered professional support for our project, opening up valuable collaboration and mentoring opportunities.\nSome photos from the event participation [Add your photos here] Overall, the event not only provided technical knowledge but also changed my mindset regarding application design, system modernization, and effective team collaboration.\n"},{"uri":"https://dokang307.github.io/Internship_Report/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://dokang307.github.io/Internship_Report/tags/","title":"Tags","tags":[],"description":"","content":""}]